{"Blog/Implementing-MSE":{"slug":"Blog/Implementing-MSE","filePath":"Blog/Implementing MSE.md","title":"Implementing MSE","links":[],"tags":[],"content":"Mean square error is defined as\n\\text{MSE}(Y, \\hat Y) = \\frac{1}{n} \\sum^{n}_{i = 1}{(Y_i - \\hat Y_i)^2},\n$$where both $Y$ and $\\hat Y$ is a 1D array of values. The loss is always positive, the larger the difference from truth, the bigger the penalty is and it&#039;s easily differentiable.\n\nWe are interested in the derivative given the prediction $\\hat Y$. The derivative for the $k$-th label is  \n\\frac{\\partial \\text{MSE}(Y, \\hat Y)}{\\partial \\hat Y_k} =\n-\\frac{2}{n} (Y_k - \\hat Y_k).\n\n\\nabla \\text{MSE}(Y, \\hat Y) = -\\frac{2}{n}(Y-\\hat Y) = \\frac{2}{n}(\\hat Y - Y).\n$$ Some people like to omit the \\frac{2}{n}. While optimizing, the factor doesn’t really matter - meaning the optimum stays the same.\nImplementation\nLet’s jump directly into the code\nimport numpy as np\n \nclass MSELoss():\n\tdef forward(self, y, y_pred):\n\t\tassert len(y.shape) == 1 and len(y_pred.shape) == 1, &quot;Not a 1D array.&quot;\n\t\tassert y.shape == y_pred.shape, &quot;Dimension mismatch&quot;\n\t\treturn np.mean(np.power(y - y_pred, 2))\n\t\n\tdef backward(self, y, y_pred):\n\t\tassert len(y.shape) == 1 and len(y_pred.shape) == 1, &quot;Not a 1D array.&quot;\n\t\tassert y.shape == y_pred.shape, &quot;Dimension mismatch&quot;\n\t\tn = y.shape[0]\n\t\treturn (2.0 / n) * (y_pred - y)"},"Blog/Implementing-activation-functions":{"slug":"Blog/Implementing-activation-functions","filePath":"Blog/Implementing activation functions.md","title":"Implementing activation functions","links":[],"tags":[],"content":"ReLU\nRectified Linear unit is the most basic activation function. For each element in the input vector, it returns 0 if the value is negative, otherwise it returns the value itself.\n\\text{ReLU}(x) = \\max(0, x)The forward message can be rewritten as \\max(0, x) = \\begin{cases} x \\quad x \\ge 0 \\\\ 0  \\quad \\text{else}\\end{cases} = \\frac{|x|+x}{2}.The python code in numpy can therefore be either\ndef forward(self, X):\n\tself.X = X\n\treturn X * (X &gt; 0)\nor\ndef forward(self, X):\n\tself.X = X\n\treturn (np.abs(X) + X) / 2\nHowever, first version will almost surely result in less operations, though it’s not as fancy. Notice that we need to store the input matrix X for the backward message. Or better, store only the boolean mask (X &gt; 0).\nFor the backward message, activation function doesn’t have any parameters, but we still need a backward message for previous layers. For a single sample input, the derivative with respect to x is given by \\frac{\\partial \\max(0, x)}{\\partial x} = \\begin{cases} 1 \\quad x &gt; 0 \\\\ 0  \\quad \\text{else}\\end{cases}.The full chain rule \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial \\text{ReLU}(x)}\\cdot \\frac{\\partial \\text{ReLU}(x)}{\\partial x}results in\ndef backward(self, dY):\n\treturn dY * (self.X &gt; 0)\nIn all the cases, the x &gt; 0 is performed element-wise. So is the * operator. Beware that at 0, the gradient technically doesn’t exists. However, it is standard to set it to 0(as this implementation does)."},"Blog/Implementing-linear-layer":{"slug":"Blog/Implementing-linear-layer","filePath":"Blog/Implementing linear layer.md","title":"Implementing linear layer","links":["Blog/Training-a-neural-network"],"tags":[],"content":"Linear layer in artificial neural networks is an implementation of\ny^T = x^TW + b^T,\n$$where \n- $y  \\in \\mathbb{R}^{\\text{n\\_outputs} \\times 1}$ is the layers output, \n- $x  \\in \\mathbb{R}^{\\text{n\\_inputs} \\times 1}$ is an input vector, \n- $W \\in \\mathbb{R}^{\\text{n\\_inputs} \\times \\text{n\\_outputs}}$ is the weight matrix and\n- $b \\in \\mathbb{R}^{\\text{n\\_outputs} \\times 1}$ is the bias.\n## Learning the layer\n\nIn order to learn the network, we will need to compute gradients in the future. Gradients are necessary in the **stochastic gradient descent** used for training. To compute gradients for an arbitrary neural network, we use the **back-propagation algorithm**. This algorithm needs 2 functions - `forward` and `backward`.\n### Forward pass\nForward pass computes a **forward message**, which is the output of the linear layer. We can use `numpy` to compute the forward message for a batch of inputs described as a matrix $X$. \n```python\nreturn np.dot(X, self.W) +  self.b\n```\nWhat the code actually does, is computing \nY = XW + B = \\begin{bmatrix} x_1^T W + b^T \\ x_2^T W + b^T \\  \\vdots\\ x_{\\text{n_samples}}^T W + b^T \\end{bmatrix}, \\qquad \\text{where } X = \\begin{bmatrix} x_1^T \\ x_2^T \\ \\vdots\\ x_{\\text{n_samples}}^T\\end{bmatrix}, B = \\begin{bmatrix} b^T \\ b^T \\ \\vdots\\ b^T\\end{bmatrix}.\n$$The output dimensions of  Y are therefore  {\\text{n\\_samples} \\times \\text{n\\_outputs}}. Note how the innocent looking + self.b does what numpy calls broadcasting.\nBackward pass\nFollowing what we derived in Training a neural network, backward pass computes the partial derivatives using following layers backward message \\delta and cached input X. We need to derive partial derivative w.r.t. weights W, bias b and we need backward message for other layers.\n\\frac{\\partial Y_{ij}}{\\partial W_{mn}}\nself.dW = np.dot(self.X.T, dY) / self.X.shape[0]\nself.db = dY.mean(axis=0)\n \ndX = np.dot(dY, self.W.T)\nInitialization\nscale = np.sqrt(2.0 / self.n_inputs)\nself.W = self.rng.normal(0.0, scale, (self.n_inputs, self.n_outputs))\nself.b = np.zeros(self.n_outputs)\nscale = np.sqrt(1.0 / self.n_inputs)\nself.W = self.rng.normal(0.0, scale, (self.n_inputs, self.n_outputs))\nself.b = np.zeros(self.n_outputs)"},"Blog/Training-a-neural-network":{"slug":"Blog/Training-a-neural-network","filePath":"Blog/Training a neural network.md","title":"Training a neural network","links":[],"tags":[],"content":"Stochastic gradient descent(SGD) is the fundamental algorithm for training an artificial neural network(ANN). Although there are many different types of artificial neural networks, a lot of them can be trained using a gradient based optimization. Let’s look at some pseudo-codes first.\nStochastic Gradient descend\n\\begin{aligned}\n{}\\ &amp;\\text{Input: } \\theta_0,\\ \\text{learning rate } \\eta \\\\[4pt]\n{}\\ &amp;\\text{for } t = 0,1,2,\\dots \\text{ do:} \\\\\n{}\\ &amp;\\quad g_t = \\nabla f(\\theta_t) \\\\\n{}\\ &amp;\\quad \\theta_{t+1} = \\theta_t - \\eta g_t\n\\end{aligned}\nThis algorithm is one of many gradient based algorithms used for training neural networks. There are only two core steps in these algorithms:\n\nCompute gradient of loss with respect to networks parameters,\nSubtract a fraction of the gradient from current parameters.\nThese steps are repeated for fixed number of steps, or until there is a change(which is usually described as “until converges”). This leads to a general framework you can find in the PyTorch implementation.\n\nfor epoch in 1..epochs:\n\n\tbatch = random sample from X and y\n\t\n\tpredictions = current prediction from the network\n\tloss = error computed from prediction and true label\n\t\n\tgradients = list of partial derivative for each parameter in network\n\t\n\tfor p in network.parameters:\n\t\tp -= learning_rate * gradients[p]\n\nComputing the gradient\nA neural network can be written as a composition of functions:\nf_N \\circ f_{N-1} \\circ \\dots \\circ f_1 (X) = f_N(\\dots(f_2(f_1(X, \\theta_1), \\theta_2)\\dots), \\theta_N),  \nwhere f_i is the i-th layer and \\theta_1, \\theta_2, \\dots, \\theta_M are the parameters of the network. The parameters are tight to each layer. The network output is\ny_\\text{pred} = f_N \\circ \\dots \\circ f_1(X)\nand the loss function is\n\\mathcal{L}(y_\\text{pred}, y) = \\mathcal{L}\\big(f_N \\circ \\dots \\circ f_1(X), y\\big)\nwhich can be seen as adding another “layer” with a fixed parameter y.\nBackpropagation\nThe main idea behind the backpropagation algorithm is to use chain rule, which is just applying the composite derivative rule (f\\circ g)&#039;(x) = f&#039;(g(x))\\cdot g&#039;(x) over and over again.\nFirst, we compute the forward message by passing through the network and caching results in each layer. To obtain partial derivative for parameter \\theta_k in layer k, we need to compute\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k} = \\frac{\\partial \\mathcal{L}}{\\partial f_N} \\cdot \\frac{\\partial f_N}{\\partial f_{N-1}} \\cdots \\frac{\\partial f_{k+2}}{\\partial f_{k+1}} \\cdot \\frac{\\partial f_{k+1}}{\\partial \\theta_k}  \n= \\frac{\\partial \\mathcal{L}}{\\partial f_{k+1}} \\cdot\\frac{\\partial f_{k+1}}{\\partial \\theta_k}\nwhich is called the parameter message. To compute the parameter message, we need to compute the backward message\n\\frac{\\partial \\mathcal{L}}{\\partial f_{k+1}}\nfor which we need, by the same chain rule, backward messages for all the k+2, k+3, \\dots, N layers. The loss derivative has no parameters and can be computed using only the prediction value y_\\text{pred}. We set it as \\delta_\\mathcal{L} and use it to initiate the algorithm. For each layer k, we only need to know how to compute the derivative given the numerical value of following layers derivative \\delta_{k+1} and the layers input X.\nFramework outline\nThe following Layer python code can be used for layers. The math is illustrative.\nclass Layer():\n\tdef __init__(self):\n\t\t# initialize weights\n\t\t# check layer parameters\n\t\tself.params = [...]\n\t\tself.grads = [...] # zeroes for each parameter\n\t\t\n\tdef forward(self, X):\n\t\t&quot;&quot;&quot;Compute forward message and cache input X&quot;&quot;&quot;\n\t\tself.X = X # caching the input\n \n\t\t# compute layer for input X\n\t\treturn X + 1\n\t\t\n\tdef backward(self, delta):\n\t\t&quot;&quot;&quot;Compute the backward and parameter message.&quot;&quot;&quot;\n\t\t\n\t\t# compute parameter message\n\t\tfor i in range(len(self.params)):\n\t\t\tself.grads[i] = max(0, self.params[i] * self.X * delta)\n\t\t\n\t\t# compute and return the backward message\n\t\tdX = self.X * delta\n\t\treturn dX\n\t\n\tdef parameters(self):\n\t\t&quot;&quot;&quot;Generator to return parameters&quot;&quot;&quot;\n\t    for idx in range(len(self.params)):\n\t        yield self.params[idx], self.grads[idx]\n \nThe network can use the same template with forward and backward, but (minimal) network would typically look like\nclass Network():\n    def __init__(self):\n        # define the layers\n        self.layer1 = ...\n        self.layer2 = ...\n        self.layers = [self.layer1, self.layer2]\n \n    def forward(self, X):\n        &quot;&quot;&quot;Forward pass through the whole network&quot;&quot;&quot;\n        for layer in self.layers:\n            X = layer.forward(X)\n        return X\n \n    def backward(self, delta):\n        &quot;&quot;&quot;Backward pass through the network&quot;&quot;&quot;\n        # propagate backward through layers in reverse\n        for layer in reversed(self.layers):\n            delta = layer.backward(delta)\n        return delta\n \n    def _parameters(self):\n\t\t&quot;&quot;&quot;Yield all parameters from all layers&quot;&quot;&quot;\n\t\tfor layer in self.layers:\n\t\t\tif layer.has_params():\n\t\t\t\tyield from layer.parameters()\n \n\tdef parameters(self ):\n\t\t&quot;&quot;&quot;Public method returns a fresh generator&quot;&quot;&quot;\n\t\treturn self._parameters()\n \nwhile. Notice that torch doesn’t force you to implement backward for the network. The reason is that they are more clever and spend a decade to perfect it :). If you’re interested  how its done, read the Gentle Introduction to??torch.autograd.\nOptimizer\nTo finish this basic torch-like framework, we need the optimizer. The only role optimizer has is to take the computed gradients and perform an optimizer step. Outline snippet could look like this.\nclass Optimizer():\n\tdef __init__(self, params, learning_rate):\n\t\tself.params = params # could be generator or list\n\t\tself.lr = learning_rate\n \n\tdef step(self):\n\t\tfor param, grad in self.params(): # in case self.params is a generator\n\t\t\tparam -= self.lr * grad\nA word of warning - this generator syntax assumes that param is a mutable object. That is numpy array, torch tensor etc. Don’t store plain int or double, that won’t update anything.\nWrap up\nWith this framework outline, training loop created previously changes to\nimport numpy as np\nfrom math import ceil\n \n# some training data\nX = np.array([...])\ny = np.array([...])\n \nn = X.shape[0]\nrng = np.random.default_rng()\nbatch_size = ceil(n**0.5)\nnum_epochs = 1000\n \nnetwork = Network()\nloss_fn = Loss()\noptimizer = Optimizer(network.parameters, learning_rate=0.01)\n \nfor _ in range(num_epochs):\n    # sample random batch from training data\n    idx = rng.integers(0, n, size=batch_size)\n    Xb = X[idx]\n    yb = y[idx]\n \n    # get predictions\n    preds = network.forward(Xb)\n \n    # compute gradients\n    delta = loss_fn.backward(preds, yb)\n    network.backward(delta)\n \n    # update parameters\n    optimizer.step()\n \nNext time, I will use this outline to implement the precursor of neural network - the perceptron."},"Vault/Calculus/Injective,-Surjective,-Bijective":{"slug":"Vault/Calculus/Injective,-Surjective,-Bijective","filePath":"Vault/Calculus/Injective, Surjective, Bijective.md","title":"Injective, Surjective, Bijective","links":["tags/math"],"tags":["math"],"content":"math [Bijection], [Surjection], [Injection]\nFunction f(x): A \\rightarrow B is called surjective, when\n(\\forall y \\in B)(\\exists x \\in A)(f(x) = y)\n\nSurjectivity means that “every image y has at least one pre-image x“.\n\n\nFunction f(x): A \\rightarrow B is called injective, when\n(\\forall x,y \\in A)(f(x) = f(y) \\Rightarrow x = y)\n\nInjectivity means that “every image y has at most one pre-image x“.\n\n\nFunction f(x): A \\rightarrow B is called bijective, when it is both surjective and injective.\n\nWhich is usefull definition for proofs. It is sometimes defined as each image y belongs exactly to one element x. It also implies |A| = |B| for finite sets. In case of sets with infinite cardinality, existence of bijection between those sets is what defines their equal cardinality.\n\nExamples\n\nIdentity function f(x) = x is bijection.\nQuadratic function f(x) = x^2 is neither injective, nor surjective when f: \\mathbb{R} \\rightarrow \\mathbb{R}. However, it is bijective when f: (0, \\infty) \\rightarrow (0, \\infty). That is because for every y \\in \\mathbb{R}\\setminus \\{0\\}, there exists x = \\pm \\sqrt{y}.\n"},"Vault/Databases/CAP-Theorem":{"slug":"Vault/Databases/CAP-Theorem","filePath":"Vault/Databases/CAP Theorem.md","title":"CAP Theorem","links":["tags/DS2"],"tags":["DS2"],"content":"DS2\nProposed by Eric Brewer. NoSQL databases can never have all of these 3 properties:\n\nConsistency (konzistentní)\nAvailability (dostupnost)\nPartition tolerance (odolnost proti výpadkům)\nassuming system is distributed and uses sharding and replication. Read and write operations are on a single aggregate\n\nConsistency\nThere is a single global order of operations; each operation takes effect at an instantaneous point between its call and completion – as if all ran sequentially on one node.\nAfter each write, all nodes must return this updated value.Weaker consistency is possible.\nAvailability\nEvery read or write request successfully received by a non-failing node in the system must result in a response (success or failure), not be silently dropped. I.e., their execution must not be rejected\nEach working node must always respond to user request\nPartition tolerance\nThe network is allowed to lose arbitrarily many messages sent from one node to another\nSuggest some network error. Nodes can get out of sync and they can lose messages. Connection error can happen, but the system must be ready.\nVariations\nIn distributed system, system can choose between Availability and Consistency. Nodes require communication, so Partition tolerance usually\nAP\nSystem is always available and it is able to re-sync after partition. But before they re-sync, nodes can give non-consistent results.\n\nDynamoDB, Cassandra, CouchDB\n\nCP\nSystem chooses to be temporarily unavailable in order to be consistent.\n\nRedis, MongoDB, ApacheHBASE\n\nCA\nConsistent and available system is possible mainly in traditional databases. Nodes will never suffer from network issues, because nodes are usually not in different networks\n\ntraditional databases (MySQL, PostgreSQL, MSQL, Redshift), neo4j, Vertica\n\nACID properties are achievable (avalability, consistency, Isolation, Durability)"},"Vault/Databases/Consistency":{"slug":"Vault/Databases/Consistency","filePath":"Vault/Databases/Consistency.md","title":"Consistency","links":["tags/DS2","Vault/Databases/CAP-Theorem"],"tags":["DS2"],"content":"DS2\nCAP Theorem\nLevels\nStrong\nWeak\n\nTypes\nWrite consistency\nRead consistency\nStrong consistency"},"Vault/Databases/Consistent-hashing":{"slug":"Vault/Databases/Consistent-hashing","filePath":"Vault/Databases/Consistent hashing.md","title":"Consistent hashing","links":[],"tags":[],"content":""},"Vault/Databases/Distribution-model":{"slug":"Vault/Databases/Distribution-model","filePath":"Vault/Databases/Distribution model.md","title":"Distribution model","links":["Vault/Databases/Sharding","Vault/Databases/Replication"],"tags":[],"content":"Specific way how Sharding and Replication are implemented. Most NoSQL system often does distribution automatically."},"Vault/Databases/Replication":{"slug":"Vault/Databases/Replication","filePath":"Vault/Databases/Replication.md","title":"Replication","links":[],"tags":[],"content":"Same data on different nodes"},"Vault/Databases/Sharding":{"slug":"Vault/Databases/Sharding","filePath":"Vault/Databases/Sharding.md","title":"Sharding","links":["Vault/Databases/Consistent-hashing"],"tags":[],"content":"Horizontal partitioning, where different data are stored on different nodes.\nSharding is not the same as partitioning.  Partition is always part of a single database instance. Sharding happens on multiple database instances.\nStrategies\nThree objectives of sharding are\n\nuniform data distribution\nbalanced workload (read and write)\nrespect physical location\nThese usually contradict each other and preferences change over time.\n\nMapping structure\nShard of an aggregate is stored in a structure that needs to be maintained. Usually some centralized index structures.\nGeneral rules\nShard is determined by rule, which should be faster. Doesn’t require to be stored.\nHash sharding\nCompute hash from a set of columns.\nCons - adding new node requires recalculating all hashes (solution lies in Consistent hashing.\nRange based sharding\nEach shard hold a range of values. e.g.\n\n1-20, 21-40, 41-60, …\nA-C, D-F, G-I, …\n\nDirectory-based sharding\nDedicated server stores information about each shard. Upon requesting data, node asks this server which shard it lies in.\nEntity/relationship-based sharding\nYou create a network and create shard based on strong components\n\nyour friends, messages and posts\n\nGeograhy-based sharding\nCloser = faster, although it separates users from different locations.\nFunctional sharding\nData are separated based on some logic, their purpose (orders, users, messages, …)\nTime-based sharding\nEach shard contains data for a specific time (for example, a day, week, month, or year).\nCombined sharding\n\nExample A banking system uses geographic sharding to distribute data by region, and within each region, range sharding is applied based on account numbers.\n"},"Vault/Databases/Types-of-databases/Column/Cassandra/Cassandra":{"slug":"Vault/Databases/Types-of-databases/Column/Cassandra/Cassandra","filePath":"Vault/Databases/Types of databases/Column/Cassandra/Cassandra.md","title":"Cassandra","links":["tags/DS2","Vault/Databases/Types-of-databases/Column/Column"],"tags":["DS2"],"content":"DS2\nType of Column NoSQL database.\n\nMain page: cassandra.apache.org/\nUse cases of big companies: cassandra.apache.org/_/case-studies.html\n\nCassandra is distributed database, meaning it consist of multiple nodes. Node is each instance of Cassandra application.\nArchitecture is masterless. Nodes by default communicate (Oracle sexists call it gossip) and each node can temporarily become a coordinator. Coordinator is the first node receiving a request, making sure the message is distributed to all other nodes.\nEach node has limit of 1TB by default(best empirical pracitce). To achieve more capacity, increase number of nodes. Cassandra scales linearly. Doubling the number of nodes leads to double the capacity at the same speed (networking is the only bottleneck)\nTo ensure fault tolerance, user can set RF - Replication factor. RF describes how many copies of your data should exist in the database.\nBy default, Cassandra is an AP (Available, Partition-tolerant) system, but it allows tunable consistency via the Consistency Level (CL) parameter. CL defines the minimum number of replicas that must acknowledge a read or write before the coordinator responds. A common setting is QUORUM = (Replication Factor / 2) + 1, which provides a good balance between consistency and availability. There are different CL options like ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM.\nSyntax and functions\nIt uses syntax close to SQL called CQL(Cassandra query language). Two main types of functions are Scalar and Aggregate functions.\ncassandra.apache.org/doc/latest/cassandra/developing/cql/functions.html\nAggregate\nAggregate multiple rows resulting from a SELECT statement.  Values for each row are input, to return a single value for the set of rows aggregated.\n\nNative aggregates\nCount, Max, Min, Sum, Avg\nUser defined\nPart of cassandra schema, are automatically propagated to all nodes\n\nScalar\nTake a number of values and produce an output.\nRetrieving the current date/time\ncurrent_timestamp, current_date, current_time, current_timeuuid\nTime conversion functions\nto_timestamp, to_date, to_unix_timestamp\nBlob conversion functions\nBlob is an arbitrary sequence of bytes.\nabs, exp, log, log10, round\nCollection functions\nAggregate functions for collections. Collections are maps, sets and lists stored as value in a column.\nmap_values, map_keys, collection_count,collection_min,collection_max,collection_sum,collection_avg\nData masking functions\nAdds extra security by replacing certain values by a nonsense or hash.\nmask_default(value), mask_null(value),  mask_replace(value, replacement]),mask_inner(value, begin, end, [padding]),mask_outer(value, begin, end, [padding]),mask_hash(value, [algorithm])\nVector similarity functions\nThey seem to be preparing for LLM training lol.\nsimilarity_cosine(vector, vector), similarity_euclidean(vector, vector), similarity_dot_product(vector, vector)\nWorkstyle\nIn Cassandra, there are no joins, no subqueries, no foreign keys. Tables must be designed for specific queries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptRelational DBCassandraDesign goalNormalize data (reduce duplication)Denormalize data (optimize queries)RelationshipsJoins, foreign keysDuplicated data across tablesQuery flexibilitySchema first, queries laterQueries first, schema followsSchemaFixed columns for all rowsFlexible — each row can have different columnsTransactionsFull ACIDLimited (eventual consistency)ScalingVertical (bigger machine)Horizontal (add nodes)\nTable design checklist\n\nIdentify your queries first – schema must answer specific access patterns.\nChoose partition key – determines which node stores data (load balancing).\nAdd clustering columns – define ordering of rows inside partition.\nAvoid large partitions – too many rows under one key = slow reads.\nDuplicate data if needed – separate tables for different queries.\nPrefer sequential writes – Cassandra optimized for append operations.\n"},"Vault/Databases/Types-of-databases/Column/Column":{"slug":"Vault/Databases/Types-of-databases/Column/Column","filePath":"Vault/Databases/Types of databases/Column/Column.md","title":"Column","links":["tags/DS2","Vault/Databases/Types-of-databases/Column/Cassandra/Cassandra"],"tags":["DS2"],"content":"DS2\nThere are N rows. Each row is created as a set of M_N columns.\nColumn consists of column name and values.\nPros\n\nFast write\nIn case of known column key fast read\n\nCons\n\nBad at aggreation\n\nApplication\n\nTime series database\nbig data large amount of writes\nLogging\n\nExamples\nCassandra"},"Vault/Databases/Types-of-databases/Document/Document":{"slug":"Vault/Databases/Types-of-databases/Document/Document","filePath":"Vault/Databases/Types of databases/Document/Document.md","title":"Document","links":["tags/DS2","Vault/Databases/Types-of-databases/Document/MongoDB"],"tags":["DS2"],"content":"DS2\nHave hierarchical inner structure. Like XML or JSON. Documents organized into collections.\nExamples\nMongoDB"},"Vault/Databases/Types-of-databases/Document/MongoDB":{"slug":"Vault/Databases/Types-of-databases/Document/MongoDB","filePath":"Vault/Databases/Types of databases/Document/MongoDB.md","title":"MongoDB","links":["tags/DS2"],"tags":["DS2"],"content":"DS2"},"Vault/Databases/Types-of-databases/Graph/Graph":{"slug":"Vault/Databases/Types-of-databases/Graph/Graph","filePath":"Vault/Databases/Types of databases/Graph/Graph.md","title":"Graph","links":["tags/DS2"],"tags":["DS2"],"content":"DS2\nNot distributed. Graph is hard to distribute. It consists of nodes and relations, both have properties(like timestamp, name, etc). It ca be directed or undirected\nPros\nCons\n\nAnalytics\n\n"},"Vault/Databases/Types-of-databases/Key-value/Key-value":{"slug":"Vault/Databases/Types-of-databases/Key-value/Key-value","filePath":"Vault/Databases/Types of databases/Key-value/Key-value.md","title":"Key-value","links":["tags/DS2"],"tags":["DS2"],"content":"DS2\nPractically one large hashtable inside RAM. Value can be whatever - e.g. bitmap.\nPros\n\nSpeed\nSimple model = Small complexity\n\nCons\n\nAbsence of data filtering (unless extension - Redis stack). Data filtering must be done after retrieving data in separate app.\n\nWhere to use\n\nIn need of faster response (caching)\nKey oriented queries, not data oriented queries\nShouldn’t be primary database - mostly because its volatile\n\n\nRedis"},"Vault/Databases/Types-of-databases/Key-value/Redis":{"slug":"Vault/Databases/Types-of-databases/Key-value/Redis","filePath":"Vault/Databases/Types of databases/Key-value/Redis.md","title":"Redis","links":["tags/DS2"],"tags":["DS2"],"content":"DS2\nredis.io/docs/latest/develop/data-types/\nString (base) value\n\nGET, SET, DEL\n\nHash set\n\nHGET\nHSET\nHKEYS hashmap(list of keys)\nHVALS hasHmap(list of values)\nHDEL hashmap key - delete value by key\nHLEN hashmap - length of HMAP\n\nSorted set\nredis.io/docs/latest/develop/data-types/sorted-sets/\n0 and -1 means from element index 0 to the last element (-1 works here just as it does in the case of the LRANGE command)."},"Vault/Databases/Types-of-databases/Types-of-NoSQL-databases":{"slug":"Vault/Databases/Types-of-databases/Types-of-NoSQL-databases","filePath":"Vault/Databases/Types of databases/Types of NoSQL databases.md","title":"Types of NoSQL databases","links":["tags/DS2","Vault/Databases/Types-of-databases/Key-value/Key-value","Vault/Databases/Types-of-databases/Column/Column","Vault/Databases/Types-of-databases/Document/Document","Vault/Databases/Types-of-databases/Graph/Graph"],"tags":["DS2"],"content":"DS2\n\nKey-value\nColumn\nDocument\nGraph\n\nBut also\n\nnative XML DB\nRDF\n"},"Vault/Databases/tutorial-3":{"slug":"Vault/Databases/tutorial-3","filePath":"Vault/Databases/tutorial 3.md","title":"tutorial 3","links":["tags/DS2"],"tags":["DS2"],"content":"DS2\n\nPossible exam question - design sharding strategy based on the planned task.\n"},"Vault/Empirical-risk-minimization":{"slug":"Vault/Empirical-risk-minimization","filePath":"Vault/Empirical risk minimization.md","title":"Empirical risk minimization","links":["tags/SSU","Vault/Hoeffding-inequality"],"tags":["SSU"],"content":"SSU\nGiven a training set T_m, learn a predictor that minimizes the expected risk.\nHowever, we only have the empirical risk. So we gather enough samples, and find a classifier, that minimizes empirical risk.\nNumber of samples needed for a given loss function and training set can be estimated using Hoeffding inequality.\n\nHowever, ERM does not tell us if the used method for a given problem is correct. The theory only gives us hope, that using empirical risk as a substitute for true risk is enough. However, if the true risk is high (= method is not suitable), we can train how much we want, but we’ll never train a good predictor by simply training ERM. Overfitting = empirical risk is much lower than true risk.\n\n\nTakeaway - domain knowledge is important when selecting a learner.\n"},"Vault/Gradient-descent":{"slug":"Vault/Gradient-descent","filePath":"Vault/Gradient descent.md","title":"Gradient descent","links":["tags/SSU","Optimization","Gradient","Vault/Statistical-machine-learning/Neural-networks/Backpropagation"],"tags":["SSU"],"content":"SSU | [GD]\nOptimization algorithm solving solving \\text{argmin}_{\\theta}\\space \\mathcal{L}(\\theta) by moving through the parameter space steps given by the gradient.\nEach step is given by\\mathbf{\\theta}_{k+1} = \\mathbf{\\theta}_{k} - \\alpha_k \\nabla\\mathcal{L}(\\mathbf{\\theta}_{k}), where \\alpha_k  is called the learning rate or step size at iteration k. The learning rate exist in order to prevent algorithm get stuck at local minimum.\nBackpropagation is the standart way to compute gradient \\nabla\\mathcal{L}(\\mathbf{\\theta}_{k}).\nProperties"},"Vault/Hoeffding-inequality":{"slug":"Vault/Hoeffding-inequality","filePath":"Vault/Hoeffding inequality.md","title":"Hoeffding inequality","links":["tags/SSU","Vault/Statistics/Independent-data","Expected-value"],"tags":["SSU"],"content":"SSU\nLet there be a series of n i.i.d. samples generated from a random variable with distribution q(z). Let the sampled values all be in an interval [a,b]. For every \\varepsilon &gt; 0 holds an inequality \\mathbb{P}(|\\mu - \\hat\\mu_n|\\ge \\varepsilon) \\le 2 \\exp(-\\frac{2n \\varepsilon^2}{(b-a)^2}), where \\mu is the Expected value of z and \\hat\\mu_n sample mean.\nProperties\n\nholds for any bounded i.i.d. samples\ngoes to 0 as n approaches infinity\neasy to compute\nnot tight\n"},"Vault/Pokročilá-algoritmizace/Abstract-data-types/Priority-queue":{"slug":"Vault/Pokročilá-algoritmizace/Abstract-data-types/Priority-queue","filePath":"Vault/Pokročilá algoritmizace/Abstract data types/Priority queue.md","title":"Priority queue","links":["tags/PAL","Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Heap"],"tags":["PAL"],"content":"PAL\nHeap"},"Vault/Pokročilá-algoritmizace/Combinatorics/Gray-code":{"slug":"Vault/Pokročilá-algoritmizace/Combinatorics/Gray-code","filePath":"Vault/Pokročilá algoritmizace/Combinatorics/Gray code.md","title":"Gray code","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL Definice Grayův kód, Reflected binary code\nBinary numeral system, where two successive values differ only in one bit.\nDefinition of the code is recursive\n\nG^0 = (0, 1)\nG^n = (0G^{n-1}_0, 0G^{n-1}_1, 0G^{n-1}_2, \\dots,1G^{n-1}_2, 1G^{n-1}_1, 1G^{n-1}_0)\n\nOr in simple words - copy the last sequence, create its reverse. prepend 0 to the whole first sequence, prepend 1 to the reverse sequence and concatenate them together.\n\nUsed in rotary discs\n"},"Vault/Pokročilá-algoritmizace/Combinatorics/Permutations-ranking":{"slug":"Vault/Pokročilá-algoritmizace/Combinatorics/Permutations-ranking","filePath":"Vault/Pokročilá algoritmizace/Combinatorics/Permutations ranking.md","title":"Permutations ranking","links":["tags/PAL"],"tags":["PAL"],"content":"PAL"},"Vault/Pokročilá-algoritmizace/Combinatorics/Ranking-function":{"slug":"Vault/Pokročilá-algoritmizace/Combinatorics/Ranking-function","filePath":"Vault/Pokročilá algoritmizace/Combinatorics/Ranking function.md","title":"Ranking function","links":["tags/Definice","tags/math","Vault/Calculus/Injective,-Surjective,-Bijective"],"tags":["Definice","math"],"content":"Definice math\nA bijection assigning unique number to each element of set S. \\text{rank}:S \\rightarrow \\{0, 1, \\dots, |S|-1\\}Unrank function is an inverse of the ranking function.\nSuccessor function finds element of a set s \\in T, given another element of set t\\in T for which \\text{rank}(s) =\\text{rank}(t) + 1."},"Vault/Pokročilá-algoritmizace/Combinatorics/Subset-Ranking":{"slug":"Vault/Pokročilá-algoritmizace/Combinatorics/Subset-Ranking","filePath":"Vault/Pokročilá algoritmizace/Combinatorics/Subset Ranking.md","title":"Subset Ranking","links":["tags/PAL","tags/Algoritmus","Vault/Pokročilá-algoritmizace/Combinatorics/Ranking-function"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\nRanking subsets based on significant vector. Significant vector is a binary string of lenght n, which is 1 if element is in a subset and zero otherwise.\nLet set T={1,2,3,4,5}. Rank 5 = 00101, therefore subset of T given rank 5 is {3,5}.\nRanking\nFunction SubsetLexRank(size n; set T) : rank {\n\tr = 0;\n\tfor i = 1..n:\n\t\tif i in T:\n\t\t\tr = r + 2**(n-i)\n}\n\nUnranking\nBasically converting rank to its binary representation by dividing to 2 until 0.\nFunction SubsetLexUnrank( size n; rank r ) : set {\n\tT = { }\n\tfor i = n downto 1: \n\t\tif r mod 2 = 1: \n\t\t\tT = T ∪ {i} \n\t\tr = r div 2\n\treturn T\n}\n\nFinding successor\nBasically just add one and do the unranking algorithm"},"Vault/Pokročilá-algoritmizace/Combinatorics/k-Element-Subsets-ranking":{"slug":"Vault/Pokročilá-algoritmizace/Combinatorics/k-Element-Subsets-ranking","filePath":"Vault/Pokročilá algoritmizace/Combinatorics/k-Element Subsets ranking.md","title":"k-Element Subsets ranking","links":["tags/PAL","tags/Algoritmus","Vault/Pokročilá-algoritmizace/Combinatorics/Ranking-function"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\nIterating over subset of only the length of k using Ranking function .\nLet set T={1,2,3,4,5}. All subsets of length k=3 are\n123\n124\n125\n134\n135\n145\n234\n235\n245\n345\nRank 5 = 00101, therefore subset of T given rank 5 is {3,5}.\nRanking\nFunction kSubsetLexRank(size n; k-subset of T) : rank {\n\tr = 0;\n\tT[0] = 0;\n\t\n\tfor i = 1..k:\n\t\tif T[i-1]+1 &lt; T[i]-1:\n\t\t\tfor j = T[i-1]+1..T[i]-1\n\t\t\t\tr = r + comb(n-j, k-i);\n}\n\nUnranking\nBasically converting rank to its binary representation by dividing to 2 until 0.\nFunction SubsetLexUnrank( size n; rank r ) : set {\n\tT = { }\n\tfor i = n downto 1: \n\t\tif r mod 2 = 1: \n\t\t\tT = T ∪ {i} \n\t\tr = r div 2\n\treturn T\n}\n\nFinding sucessor\nfunction KSubsetLexSuccesor(k-element subset of T, size n){\n\tU = T;\n\ti = 0;\n\t\n\twhile ( i &lt; k AND T[k - i] = n - i ){\n\t\ti++;\n\t}\n\t\n\ti = k - i // index in the \n\tif ( i == 0 ) throw; // \n\t\n\tfor (j = i..k){\n\t\tU[j] = T[i] + 1 + j - i\n\t}\n}\n\nImagine ABAKUS!\n1234        Right is empty, take 4, make it 5\n123 5       Right is empty, tak 5, make it 6\n123  6      Right is 6, skip it, take 3, make it 45\n12 45       Right is empty, take 5, make it 6\n12 4 6      Right is 6, skip it, make 4 into 56\n12  56      Right is 56, skip it, turn 2 into 345\n1 345 \n1 34 6\n\nLike abakus, you skip the right “piece”, find the first from the right, push it one right and take all the rest and put is right after."},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binary-heap":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binary-heap","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/Binary heap.md","title":"Binary heap","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Heap"],"tags":["PAL","Definice"],"content":"PAL Definice Binární halda\n(Minimal) Binary Heap is a tree data structure with following properties\n\nEvery level is full except possibly the last, which is filled from left to right\nif x is a node and ch a child of x, then k(x) \\ge k(ch)\n\nOperations\nInsert\nInsert (x) {\n\tAdd a node x at the end of the heap; \n\twhile ( key(parent(x)) &gt; key(x) ) { \n\t\tSwap node x with parent(x); \n\t}\n}\n\nThe while loop is sometimes called bubble up.\nDepth of heap is \\log(n), worst case is bubbling to the root.\nAccessing minimum\nConstant, just return root of heap.\nRemoving minimum\nSwap root with last element and bubble down.\nExtractMin(){\n\tSwap first and last node;\n\tRemove last node;\n\tx = root;\n\twhile( k(x)&gt;k(l(x)) or k(x)&gt;k(r(x)) ){\n\t\ty = least of l(x) or r(x);\n\t\tSwap x an y;\n\t\tx = y;\n\t}\n}\n\nDelete any node\nYou get a pointer to a node. So you swap it with the end. Now it violates both sides\nDelete(&amp;x){\n\tSwap contains of x and last node;\n\tRemove last node;\n\t\n\tif(k(x) &lt; k(p(x))){\n\t\tbubbleUp(x)\n\t}\n\telse{\n\t\tbubbleDown(x)\n\t}\n}\n\nDecreaseKey\nJust bubble up\nDecreaseKey(x, newKey):\n    key(x) = newKey\n    bubbleUp(x)\n\nRepresentation\nIt can be represented in a simple array, which makes it fast (cache is fast).\nIndexing can be done as follows, assuming the first index is 1:\n\nleft child - 2k+1\nright child - 2k\nparent - k / 2 or k &gt;&gt; 1\n"},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Heap":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Heap","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/Binomial Heap.md","title":"Binomial Heap","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Tree"],"tags":["PAL","Definice"],"content":"PAL Definice Binomiální halda\nA data structure made of Binomial Trees\nOperations\nMerge\nMerging trees of the same order is trivial, following the definition.\nMerging heaps requires merging trees from lowest order to highest and potentially taking care of a “carry” tree.\nMergeHeaps(X, Y) {\n    result = empty heap\n    C = null  // carry tree\n    \n    for d = 0 to max(maxOrder(X), maxOrder(Y)) {\n        A = tree of order d from X (or null)\n        B = tree of order d from Y (or null)\n        count = number of non-null trees among {A, B, C}\n        \n        if (count == 1) {\n            Add the non-null tree to result\n            C = null\n        }\n        else if (count == 2) {\n            C = MergeTree(the two non-null trees)\n        }\n        else if (count == 3) {\n            Add tree with smallest root to result\n            C = MergeTree(other two trees)\n        }\n    }\n    \n    if (C != null) add C to result\n    Update min pointer by scanning all roots\n    return result\n}\n"},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Tree":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Tree","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/Binomial Tree.md","title":"Binomial Tree","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL Definice Binomiální strom\nRecursively defined as following:\n\nBinomial tree of order 0 is a single node.\nBinomial tree of order k is made of two trees of order k-1. Tree with lesser key in root has the second tree linked as descendant.\n\nAlternatively:\n\nBinomial tree of order 0 is a single node.\nBinomial tree of order k is made of a new node with trees of order 0,1, 2, \\dots, k-1 linked as descendants in that order\n\nProperties\nk-order binomial tree has\n\n2^k nodes\n\\binom{k}{i} nodes in the i-th level\ndepth k\n"},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/D-ary-heap":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/D-ary-heap","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/D-ary heap.md","title":"D-ary heap","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binary-heap"],"tags":["PAL","Definice"],"content":"PAL Definice D-nární halda\nThis is analogical to Binary heap, but there are N descendants. The complexities remain the same, but for them to be efficient, use powers of two.\nA d-ary heap typically runs much faster than a binary heap for heap sizes that exceed the size of the computer’s cache memory."},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Fibonacci-Heap":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Fibonacci-Heap","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/Fibonacci Heap.md","title":"Fibonacci Heap","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Binomial-Heap"],"tags":["PAL","Definice"],"content":"PAL Definice Fibonacciho halda\nLoosely based on Binomial Heap, but with more relaxed constrains.\nRepresentation\nFibonacci Tree\n\nWe can cut at most one child of each non-root node.\nWhen a second child is cut, the node itself needs to be cut from its parent and becomes the root of a new tree.\n\nStructure\nDoubly linked list of node,\noperations\nMerge\nConnect doubly cyclic linked list and update pointer to min\n\n\n\n"},"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Heap":{"slug":"Vault/Pokročilá-algoritmizace/Data-Structures/Heap/Heap","filePath":"Vault/Pokročilá algoritmizace/Data Structures/Heap/Heap.md","title":"Heap","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL Definice halda\nA (minimal) heap is a forest data structure where each node contains a key (and optionally a value) and satisfies the heap property: if B is a child of A, then k(B) \\ge k(A).\noperations\nHeap usually supports these operations\n\nInsert(x) adds a new key x to the heap.\nAccessMin finds and returns the minimum item of the heap.\nDeleteMin removes the minimum node of the heap\nDecreaseKey (x,d) decreases x key within the heap by d.\nMerge (H1, H2) joins two heaps H1 and H2 to form a valid new heap containing all the elements of both.\nDelete (x) removes a key x of a heap.\n"},"Vault/Pokročilá-algoritmizace/Graph-algorithms/Certificate":{"slug":"Vault/Pokročilá-algoritmizace/Graph-algorithms/Certificate","filePath":"Vault/Pokročilá algoritmizace/Graph algorithms/Certificate.md","title":"Certificate","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Graph-algorithms/Graph-invariant","Vault/Pokročilá-algoritmizace/Graph-algorithms/Isomorphism"],"tags":["PAL","Definice"],"content":"PAL Definice Certifikát\nStronger variant of Graph invariant\nCertificate for a family of graphs \\mathcal{F} is a function \\forall\\, G_1, G_2 \\in \\mathcal{F} : \\;\n\\operatorname{Cert}(G_1) = \\operatorname{Cert}(G_2)\n\\iff\nG_1 \\text{ is isomorphic to } G_2.\n\nFastest algorithms for graphs Isomorphism use certificate\n"},"Vault/Pokročilá-algoritmizace/Graph-algorithms/Graph-invariant":{"slug":"Vault/Pokročilá-algoritmizace/Graph-algorithms/Graph-invariant","filePath":"Vault/Pokročilá algoritmizace/Graph algorithms/Graph invariant.md","title":"Graph invariant","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Graph-algorithms/Isomorphism"],"tags":["PAL","Definice"],"content":"PAL Definice Invariant grafu\nFunction \\Phi over a graph, for which following holds: G \\text{ is isomorphic with }H \\Rightarrow  \\Phi(G) = \\Phi(H)\nExamples\n\nnumber of node\nnumber of vertices\nsum of unique degrees (both in and out), sorted\nsequence of degrees on a path\n\nIf invariant doesn’t hold, graphs are not isomorphic."},"Vault/Pokročilá-algoritmizace/Graph-algorithms/Isomorphism":{"slug":"Vault/Pokročilá-algoritmizace/Graph-algorithms/Isomorphism","filePath":"Vault/Pokročilá algoritmizace/Graph algorithms/Isomorphism.md","title":"Isomorphism","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL Definice\nTwo graphs G1=(V1,E1) and G2=(V2,E2) are isomorphic if there is a bijection f : V1 → V2 such that ∀ x, y ∈ V1 : { f (x), f (y) } ∈ E2 ⇔ { x, y } ∈ E1 The mapping f is said to be an isomorphism between G1 and G2."},"Vault/Pokročilá-algoritmizace/Graph-algorithms/Tree-Certificate":{"slug":"Vault/Pokročilá-algoritmizace/Graph-algorithms/Tree-Certificate","filePath":"Vault/Pokročilá algoritmizace/Graph algorithms/Tree Certificate.md","title":"Tree Certificate","links":["tags/PAL"],"tags":["PAL"],"content":"PAL Certifikát Stromu\nBuild certificate\nAssign string 01 to each node.\nWhile (number of vertices is higher than 2) {\n\tFor each non-leaf vertex {\n\t\tRemove inital 0 and trailing 1 from x&#039;s label\n\t\t\n\t\t// add labels of x and all the leaves to list\n\t\tlabels = [ label[x] ]\n\t\tfor each leaf l adjacent to x:\n\t\t\tlabels.append(label[l])\n\t\t\tremove l from graph;\n\t\t\n\t\t// sort lexicographically\n\t\tlabels = labels.lexiographicall_sort()\n\t\t\n\t\t// set new string for x\n\t\tlabel[x] = 0 + concat + 1\n\t}\n}\n\nif(only vertex x left){\n\treturn label[x]\n}\nelse(vertex x and y left){\n\treturn concat(lex_sort(label[x], label[y]))\n}\n\nBuild tree from certificate\nroot = node()\n|V| = |Cert| / 2 // there are as man 0 as 1\n\nnode = root\nforeach char c in the Cert[1]..Cert[-2]:\n\tif c == 0:\n\t\t// append new node to the current node\n\t\tnew_node = node()\n\t\tparent[new_node] = node\n\t\tnode.children.append(new_node)\n\t\tnode = new_node\n\telse:\n\t\t// return to parent\n\t\tnode = parent of node\n"},"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Kruskal's-Algorithm":{"slug":"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Kruskal's-Algorithm","filePath":"Vault/Pokročilá algoritmizace/Minimální kostra grafu/Kruskal's Algorithm.md","title":"Kruskal's Algorithm","links":["tags/PAL","tags/Algoritmus"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus"},"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Minimum-spanning-tree":{"slug":"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Minimum-spanning-tree","filePath":"Vault/Pokročilá algoritmizace/Minimální kostra grafu/Minimum spanning tree.md","title":"Minimum spanning tree","links":["tags/PAL","tags/Definice","Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Spanning-tree"],"tags":["PAL","Definice"],"content":"PAL Definice\nSpanning tree which has the lowest sum of weights."},"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Spanning-tree":{"slug":"Vault/Pokročilá-algoritmizace/Minimální-kostra-grafu/Spanning-tree","filePath":"Vault/Pokročilá algoritmizace/Minimální kostra grafu/Spanning tree.md","title":"Spanning tree","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL  Definice kostra grafu\nPodgraf, který obsahuje všechny vrcholy a je strom"},"Vault/Pokročilá-algoritmizace/Shortest-Path/BFS":{"slug":"Vault/Pokročilá-algoritmizace/Shortest-Path/BFS","filePath":"Vault/Pokročilá algoritmizace/Shortest Path/BFS.md","title":"BFS","links":["tags/Algoritmus","tags/PAL"],"tags":["Algoritmus","PAL"],"content":"Algoritmus PAL"},"Vault/Pokročilá-algoritmizace/Shortest-Path/DFS":{"slug":"Vault/Pokročilá-algoritmizace/Shortest-Path/DFS","filePath":"Vault/Pokročilá algoritmizace/Shortest Path/DFS.md","title":"DFS","links":["tags/PAL","tags/Algoritmus"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus"},"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk-optimized":{"slug":"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk-optimized","filePath":"Vault/Pokročilá algoritmizace/Souvislé komponenty/DFS Walk optimized.md","title":"DFS Walk optimized","links":["tags/PAL","tags/Algoritmus"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\ninput: Graph G.\noutput: array p pointing to predecessor vertex, array d with times of vertex opening and array f with time of vertex closing\nprocedure DFS-Walk(Vertex u) { \n\tstate[u] = OPEN;\n\tfor each Vertex v in succ(u) \n\t\tif (state[v] == UNVISITED) then {\n\t\t\tDFS-Walk(v ); \n\t\t}\n\tstate[u ] = CLOSED;\n} \n\nDFS-Walk-push(Vertex u) {\n\tstate[u] = OPEN; \n\td[u] = ++time;\n\tfor each Vertex v in succ(u)\n\t\tif (state[v ] == UNVISITED) then {\n\t\t\tp[v] = u; \n\t\t\tDFS-Walk-push(v); \n\t\t}\n\tstate[u] = CLOSED; \n\tf[u] = ++time; \n\tpush u to S;\n}\n"},"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk":{"slug":"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk","filePath":"Vault/Pokročilá algoritmizace/Souvislé komponenty/DFS Walk.md","title":"DFS Walk","links":["tags/PAL","tags/Algoritmus"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\ninput: Graph G.\noutput: array p pointing to predecessor vertex, array d with times of vertex opening and array f with time of vertex closing\nprocedure DFS-Walk(Vertex u) { \n\tstate[u] = OPEN; \n\td[u] = ++time;\n\tfor each Vertex v in succ(u ) \n\t\tif (state[v] == UNVISITED) then {\n\t\t\tp[v] = u; \n\t\t\tDFS-Walk(v ); \n\t\t}\n\tstate[u ] = CLOSED; f[u] = ++time;\n}\n\nDFS-Walk-push(Vertex u) {\n\tstate[u] = OPEN; \n\td[u] = ++time;\n\tfor each Vertex v in succ(u)\n\t\tif (state[v ] == UNVISITED) then {\n\t\t\tp[v] = u; \n\t\t\tDFS-Walk-push(v); \n\t\t}\n\tstate[u] = CLOSED; \n\tf[u] = ++time; \n\tpush u to S;\n}\n"},"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/Kosaraju-Sharir-Algorithm":{"slug":"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/Kosaraju-Sharir-Algorithm","filePath":"Vault/Pokročilá algoritmizace/Souvislé komponenty/Kosaraju-Sharir Algorithm.md","title":"Kosaraju-Sharir Algorithm","links":["tags/PAL","tags/Algoritmus","Vault/Pokročilá-algoritmizace/Souvislé-komponenty/strongly-connected-components","Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk","Vault/Pokročilá-algoritmizace/Souvislé-komponenty/DFS-Walk-optimized"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\ninput: graph G = (V, E)\noutput: set of strongly connected components (sets of vertices)\nS = empty stack; \n\nwhile S does not contain all vertices do \n\tChoose an arbitrary vertex v not in S; \n\tDFS-Walk-push(v ) and each time that DFS finishes expanding a vertex u, push u onto S; \n\nReverse the directions of all arcs to obtain the transpose graph; \nwhile S is nonempty do \n\tv = pop(S); \n\tif v is UNVISITED then \n\t\tDFS-Walk(v ); \n\tThe set of visited vertices will give the strongly connected component containing v;\n\nDFS Walk\nDFS Walk optimized"},"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/Tarjan's-algorithm":{"slug":"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/Tarjan's-algorithm","filePath":"Vault/Pokročilá algoritmizace/Souvislé komponenty/Tarjan's algorithm.md","title":"Tarjan's algorithm","links":["tags/PAL","tags/Algoritmus","Vault/Pokročilá-algoritmizace/Souvislé-komponenty/strongly-connected-components"],"tags":["PAL","Algoritmus"],"content":"PAL Algoritmus\ninput: oriented graph G = (V, E)\noutput: set of strongly connected components\nstruct Node{\n\tindex\n\tlowlink   // link to other node in SCC\n\tpred      // pointer to stack predeccesor\n\tinstack   // true if node is in stack\n} \n\nprocedure push( v ) {\n\tv.pred = S; \n\tv.instack = true; \n\tS = v;\n}\n\nfunction pop( v ) {\n\tS = v.pred; \n\tv.pred = null; \n\tv.instack = false; \n\treturn v;\n}\n\nprocedure find_scc( v ) \n\tv.index = v.lowlink = ++index; \n\tpush( v ); \n\t\n\tforeach node w in succ( v ) { \n\t\tif w.index = 0 { // not yet visited \n\t\t\tfind_scc( w ); \n\t\t\tv.lowlink = min( v.lowlink, w.lowlink ); \n\t\t}\n\t\telsif w.instack {\n\t\t\tv.lowlink = min( v.lowlink, w.index ); \n\t\t}\n\t} \n\t\n\tif v.lowlink = v.index { // v: head of SCC \n\t\tSCC++ // track how many SCCs found \n\t\trepeat {\n\t\t\tx = pop( S ); \n\t\t\tadd x to current strongly connected component;\n\t\t} \n\t\tuntil x = v; \n\t\toutput the current strongly connected component;\n\t} \n}\n\nindex = 0; // unique node number &gt; 0 \nS = null; // pointer to node stack \nSCC = 0; // number of SCCs in G \n\nforeach node v in V {\n\tif v.index = 0 { // yet unvisited \n\t\tfind_scc( v ); \n\t}\n}\n"},"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/strongly-connected-components":{"slug":"Vault/Pokročilá-algoritmizace/Souvislé-komponenty/strongly-connected-components","filePath":"Vault/Pokročilá algoritmizace/Souvislé komponenty/strongly connected components.md","title":"strongly connected components","links":["tags/PAL"],"tags":["PAL"],"content":"PAL"},"Vault/Pokročilá-algoritmizace/Trails/DFS-Euler-Trail":{"slug":"Vault/Pokročilá-algoritmizace/Trails/DFS-Euler-Trail","filePath":"Vault/Pokročilá algoritmizace/Trails/DFS Euler Trail.md","title":"DFS Euler Trail","links":["tags/PAL","Vault/Pokročilá-algoritmizace/Shortest-Path/DFS","Vault/Pokročilá-algoritmizace/Trails/Euler-Trail"],"tags":["PAL"],"content":"PAL\nModified DFS which outputs Euler Trail.\nprocedure euler-trail(vertex v); { \n\tforeach vertex u in succ(v) do { \n\t\tremove edge(v,u) from graph; \n\t\teuler-trail(u); \n\t\tpush(edge(v,u)); \n\t} \n}\n"},"Vault/Pokročilá-algoritmizace/Trails/Euler-Trail":{"slug":"Vault/Pokročilá-algoritmizace/Trails/Euler-Trail","filePath":"Vault/Pokročilá algoritmizace/Trails/Euler Trail.md","title":"Euler Trail","links":["tags/Definice","tags/PAL"],"tags":["Definice","PAL"],"content":"Definice PAL\nEulerův Tah\nTrail = a path where vertices can repeat but edges cannot.\nEuler Trail contains all the vertices."},"Vault/Pokročilá-algoritmizace/Trails/Hamilton-path":{"slug":"Vault/Pokročilá-algoritmizace/Trails/Hamilton-path","filePath":"Vault/Pokročilá algoritmizace/Trails/Hamilton path.md","title":"Hamilton path","links":["tags/PAL","tags/Definice"],"tags":["PAL","Definice"],"content":"PAL Definice\nA path that visits each node once\nHamilton Trail\nA loop in a graph which visits each node only once\nNP-Hard\nTraveling salesman problem"},"Vault/Pokročilá-algoritmizace/Trails/Traveling-salesman-problem":{"slug":"Vault/Pokročilá-algoritmizace/Trails/Traveling-salesman-problem","filePath":"Vault/Pokročilá algoritmizace/Trails/Traveling salesman problem.md","title":"Traveling salesman problem","links":["tags/FamousProblem","Vault/Pokročilá-algoritmizace/Trails/Hamilton-path"],"tags":["FamousProblem"],"content":"FamousProblem\nHamilton path asks if such path exists, TSP searches for the optimal path, which happens to be Hamilton path."},"Vault/Pokročilá-algoritmizace/Vyhledávací-stromy/Binární-vyhledávací-stromy/Binary-search-tree":{"slug":"Vault/Pokročilá-algoritmizace/Vyhledávací-stromy/Binární-vyhledávací-stromy/Binary-search-tree","filePath":"Vault/Pokročilá algoritmizace/Vyhledávací stromy/Binární vyhledávací stromy/Binary search tree.md","title":"Binary search tree","links":["tags/PAL","Vault/Pokročilá-algoritmizace/Vyhledávací-stromy/Search-trees"],"tags":["PAL"],"content":"PAL\nSearch trees"},"Vault/Pokročilá-algoritmizace/Vyhledávací-stromy/Search-trees":{"slug":"Vault/Pokročilá-algoritmizace/Vyhledávací-stromy/Search-trees","filePath":"Vault/Pokročilá algoritmizace/Vyhledávací stromy/Search trees.md","title":"Search trees","links":["tags/PAL","/"],"tags":["PAL"],"content":"PAL\nObecný vyhledávací strom je zakořeněný strom s určeným pořadím synů každého vrcholu. Vrcholy dělíme na vnitřní a vnější, přičemž platí:\n\nVnitřní (interní) vrcholy obsahují libovolný nenulový počet klíčů.\nPokud ve vrcholu leží klíče x_1 &lt; . . . &lt; x_k, pak má k + 1 synů, které označíme s_0, . . . , s_k.\nKlíče slouží jako oddělovače hodnot v podstromech, čili platí: T(s_0) &lt; x_1 &lt; T(s_1) &lt; x_2 &lt; \\dots &lt; x_{k−1} &lt; T(s_{k−1}) &lt; x_k &lt; T(s_k), kde T(si) značí množinu všech klíčů z daného podstromu. Často se hodí dodefinovat x0 = −∞ a xk+1 = +∞, aby nerovnost xi &lt; T(si) &lt; xi+1 platila i pro krajní syny\n"},"Vault/Rationalism":{"slug":"Vault/Rationalism","filePath":"Vault/Rationalism.md","title":"Rationalism","links":["tags/Philosophy"],"tags":["Philosophy"],"content":"Philosophy"},"Vault/Statistical-data-analysis/2025-10-27":{"slug":"Vault/Statistical-data-analysis/2025-10-27","filePath":"Vault/Statistical data analysis/2025-10-27.md","title":"2025-10-27","links":["tags/SAN"],"tags":["SAN"],"content":"SAN\nBayes classificator\nPlugging gaussian distro into bayes formula and taking logarithm leads to discriminant score\nApply softmax to go from disc scores back to probability P(Y = k | X = x)\nEvaluation of classifiers\nConfusion matrix\n\neasy\nBad for skewed and imbalanced distributions\n\n\nRECALL = Accuracy, Specificity, Sensitivity\n\nSensitivity = TP / P\n1 - Specificity = FP / N\nParameters\nThreshold -\nROC\n\nreceiving operator characteristics\n1 is good\nSteep is good\n\nClassifiers comparasion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameAssumptionsKGood whenLogistic2Naive bayesHigh dimensionLDAreasonable gaussianQDAkNNnone, robust\nI’ve got that poisson\nA ha! That poisson on my mind\nprobability something happens k times over fixed period of time or space\n\nProbability I’ll get fucked k times in a month (close to 0)\n\nOnly one parameter, nice\nGeneralized linear models\n\nLinear predictor + Link function + particular distribution\n\n\nParty killer = particular\n\nExponential distribution family"},"Vault/Statistical-data-analysis/2025-11-23":{"slug":"Vault/Statistical-data-analysis/2025-11-23","filePath":"Vault/Statistical data analysis/2025-11-23.md","title":"2025-11-23","links":["tags/SAN"],"tags":["SAN"],"content":"SAN\nRedukce dimenzionality\nRedukce dimenzionality spočívá v nalezení variety v prostoru, ve kterém leží naše data tak, aby na oné varietě až na šum ležela. Typicky chceme najít prostor \\mathcal{T} \\in \\mathbb{R}^L, a dvě zobrazení F a f, které převádějí vektory z \\mathbb{R}^D do \\mathcal{T} a zpět.\n\nŽijeme v trojrozměrném prostoru. Naše poloha jsou tedy 3 souřadnice (střed budiž třeba slunce). Většinou nás ale zajímá naše poloha na Zeměkouli, která se dá vyjádřit pomocí zeměpisné výšky a šířky. Původní prostor je tedy \\mathbb{R}^3. \\mathcal{M} je povrch země (nepravidelný, ale zato euklidovský)\n"},"Vault/Statistical-data-analysis/Lec3":{"slug":"Vault/Statistical-data-analysis/Lec3","filePath":"Vault/Statistical data analysis/Lec3.md","title":"Lec3","links":["tags/SAN"],"tags":["SAN"],"content":"SAN\nPicking the best model\nResidual plots\nWhat to look at\nMainly to check if the F-statistics and R-squared etc are valid. Aka check if residuals are normal and heteroscedastic etc in case of multiple variables\nNormally distributed residuals are equaly distributed around some mean value\nHeteroscedastic residuals increase over some variable (not even)\nRemoving variables\nCook’s distance\nSamples with large CD are probably outliers. Outliers influence the model too much.\nVariance inflation factor (VIC)\n1 / 1 - R\nPenalty (Shrinkage)\nRidge\nRSS plus penalty - parameter lambda\nLasso"},"Vault/Statistical-data-analysis/Lecture-2":{"slug":"Vault/Statistical-data-analysis/Lecture-2","filePath":"Vault/Statistical data analysis/Lecture 2.md","title":"Lecture 2","links":["tags/SAN"],"tags":["SAN"],"content":"SAN\nT-statitstika\np-hodnota\nRSE - residual standart error = na interpretaci musíme znát doménu (není “standardizovaná”)\nF-statistika\nMultilineární regrese\nkoeficient je vlastně “průměrný přírůstek” za daný příznak\nslide 13/46 = výška a váha jsou korelované, takže ten záporný koeficient je pravděpodobně blbě.\nPrikladek\nY=X1, X1=X2\nfitnes Y=a+bX1+cX2\npd.get_dummies()\nF test\n\nStavi na tom, ze existuje vztah mezi počtem stupnu volnosti (počet vzorků) a poctem promennych\n"},"Vault/Statistical-data-analysis/Untitled":{"slug":"Vault/Statistical-data-analysis/Untitled","filePath":"Vault/Statistical data analysis/Untitled.md","title":"Untitled","links":[],"tags":[],"content":""},"Vault/Statistical-machine-learning/2025-11-04":{"slug":"Vault/Statistical-machine-learning/2025-11-04","filePath":"Vault/Statistical machine learning/2025-11-04.md","title":"2025-11-04","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nvanishing gradients\n\nWhat causes them?\nHow to initialize network just right?\nwhich activation function can help and what other problem does it cause?\n\nXavier initialization\n\nWhat is variance of w^Tx equal to?\nHow to choose variance of weights?\n\nRegularization\n\nName 5 regularization methods\n\nMAP estimate"},"Vault/Statistical-machine-learning/2025-11-11":{"slug":"Vault/Statistical-machine-learning/2025-11-11","filePath":"Vault/Statistical machine learning/2025-11-11.md","title":"2025-11-11","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nDescribe basic idea behind Maximum margin classifier.\n\nseparate linearly separable data with hyperplane.\n\nWhat properties does this hyperplane have?\nWhat are the support vectors?\nFormulate quadratic program for finding the hyperplane.\nIs the solution for Max MC unique?\nIs the solution of quadratic program formulation unique?\nLinearly inseparable\nWhat is a slack variable?\nReformulate quadratic program using slack variables.\nName hyperparameters for soft margin SVM with slack variable.\nWhat kind of loss should be used in soft margin SVM? Which loss provide convex upper bound?\nStructural Risk minimization\nj0"},"Vault/Statistical-machine-learning/Maximum-margin-classifier":{"slug":"Vault/Statistical-machine-learning/Maximum-margin-classifier","filePath":"Vault/Statistical machine learning/Maximum margin classifier.md","title":"Maximum margin classifier","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nGiven linearly separable data T_m = \\{ (x, y) \\in \\mathbb{R}^d\\times \\{-1, +1\\} \\}, finds a linear classifier\nh(x; w, b) = \n\\begin{cases}\n+1 \\quad \\text{if } \\langle x, w \\rangle+ b \\ge 0\\\\\n-1 \\quad \\text{if } \\langle x, w \\rangle + b &lt; 0\n\\end{cases}\nWe can find infinitely many classifiers minimizing the distance between the hyperplane and training data T_m, given by\ny_i\\frac{\\langle x, w \\rangle+ b }{||w||},\nwhich is an unsigned distance. Maximum margin classifier finds parameters (w^*, b^*) by maximizing the margin, where margin is the minimal distance from hyperplane.\n(w^*, b^*) = \n\\text{arg} \\max_{w \\in \\mathbb{R}^d \\setminus \\{0\\}, b \\in \\mathbb{R}}\n\\left[ \n\t\\min_{i = 1, \\cdots, m}\n\t\ty_i\\frac{\\langle x, w \\rangle+ b }{||w||}\n\\right]"},"Vault/Statistical-machine-learning/Neural-networks/Adagrad":{"slug":"Vault/Statistical-machine-learning/Neural-networks/Adagrad","filePath":"Vault/Statistical machine learning/Neural networks/Adagrad.md","title":"Adagrad","links":["tags/SSU"],"tags":["SSU"],"content":"SSU"},"Vault/Statistical-machine-learning/Neural-networks/Adam-optimizer":{"slug":"Vault/Statistical-machine-learning/Neural-networks/Adam-optimizer","filePath":"Vault/Statistical machine learning/Neural networks/Adam optimizer.md","title":"Adam optimizer","links":["tags/SSU"],"tags":["SSU"],"content":"SSU"},"Vault/Statistical-machine-learning/Neural-networks/Backpropagation":{"slug":"Vault/Statistical-machine-learning/Neural-networks/Backpropagation","filePath":"Vault/Statistical machine learning/Neural networks/Backpropagation.md","title":"Backpropagation","links":["tags/SSU"],"tags":["SSU"],"content":"SSU"},"Vault/Statistical-machine-learning/Neural-networks/RMSProp":{"slug":"Vault/Statistical-machine-learning/Neural-networks/RMSProp","filePath":"Vault/Statistical machine learning/Neural networks/RMSProp.md","title":"RMSProp","links":["tags/SSU"],"tags":["SSU"],"content":"SSU"},"Vault/Statistical-machine-learning/Neural-networks/Stochastic-Gradient-Descent":{"slug":"Vault/Statistical-machine-learning/Neural-networks/Stochastic-Gradient-Descent","filePath":"Vault/Statistical machine learning/Neural networks/Stochastic Gradient Descent.md","title":"Stochastic Gradient Descent","links":["tags/TODO"],"tags":["TODO"],"content":"TODO"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Distribution-free-PAC-lower-bound":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Distribution-free-PAC-lower-bound","filePath":"Vault/Statistical machine learning/Statistical learning theory/Distribution-free PAC lower bound.md","title":"Distribution-free PAC lower bound","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\n\nProof\nsomething something use hoeffding inequality"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Fundamental-Theorem-of-PAC-Learning":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Fundamental-Theorem-of-PAC-Learning","filePath":"Vault/Statistical machine learning/Statistical learning theory/Fundamental Theorem of PAC Learning.md","title":"Fundamental Theorem of PAC Learning","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nTheorem: For a hypothesis class \\mathcal{H} \\subset \\{-1,+1\\}^\\mathcal{X}:\nThe following are equivalent:\n\nUniform Law of Large Numbers (ULLN) holds for \\mathcal{H}\nERM is a successful PAC learner for \\mathcal{H}\n\\mathcal{H} has finite VC dimension d_{\\text{VC}}(\\mathcal{H}) &lt; \\infty\n\nSample complexity bound:\nm_{\\text{H, PAC}}(\\epsilon, \\delta) \\le C \\frac{d_{\\text{VC}} + \\log(1/\\delta)}{\\epsilon^2}"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Hoeffding's-inequality":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Hoeffding's-inequality","filePath":"Vault/Statistical machine learning/Statistical learning theory/Hoeffding's inequality.md","title":"Hoeffding's inequality","links":["tags/SSU","tags/TODO","Vault/Statistics/Markov's-Inequality"],"tags":["SSU","TODO"],"content":"SSU\nUpper bound for probability of sample mean deviation from true mean.\nP(|\\mu - \\hat\\mu_n| \\ge \\varepsilon) \\le 2e^{-\\frac{2n\\varepsilon^2}{(b-a)^2}}\nProof\nTODO\nUses Markov’s Inequality\nImplementation\nhoeffding_bound(n, thresh, upper_bound, lower_bound):\n\treturn 2 * math.exp(- 2*n*thresh**2 / (upper_bound-lower_bound)**2)"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Prediciton-error":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Prediciton-error","filePath":"Vault/Statistical machine learning/Statistical learning theory/Prediciton error.md","title":"Prediciton error","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\ndb.movies.find(\n{\nor: [\n\t\t{rating: {gte: 80}},\n{genres: {all : [ &quot;romance&quot;, &quot;drama&quot; ] } }\n\t]\n},\n{ id_ : 1, country: {slice: 2} }\n)\ndb.users.find(\n{\n“home.city”: {in: [&quot;pr&quot;, &quot;br&quot;]}, \n&quot;year&quot; : {lt: new Date(‘2025-01-01’)}\n}\n)\ndb.users.aggregate([\n{\n\t$project: { _id: 1, fr_count: {$size: &quot;$friends&quot;} }\n},\n{\n\t$\n}\n])\n"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Structural-risk-minimization":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Structural-risk-minimization","filePath":"Vault/Statistical machine learning/Statistical learning theory/Structural risk minimization.md","title":"Structural risk minimization","links":["tags/SSU","Vault/Statistical-machine-learning/Statistical-learning-theory/VC-Dimension-generalization-bound"],"tags":["SSU"],"content":"SSU\nAlgorithm based on VC Dimension generalization bound.\nAlgorithm:\n\nConstruct nested hypothesis classes:\n\nH_1 \\subset H_2 \\subset \\dots \\subset H_K\n\nApply ERM on each class:\n\nh_i = \\arg \\min_{h \\in H_i} \\hat R(T_m, h)\n\nSelect class minimizing the VC generalization bound:\n\ni^* = \\arg \\min_i \\hat R(T_m, h_i) + \\epsilon(m, H_i, \\delta)\n\nOutput h_{i^*}\n\n\nSRM balances empirical risk and model complexity (VC dimension). It operationalizes PAC-learning guarantees using finite-sample VC bounds.\n"},"Vault/Statistical-machine-learning/Statistical-learning-theory/VC-Dimension-generalization-bound":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/VC-Dimension-generalization-bound","filePath":"Vault/Statistical machine learning/Statistical learning theory/VC Dimension generalization bound.md","title":"VC Dimension generalization bound","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nTheorem (Vapnik):\nLet \\mathcal{H} \\subset \\{-1,+1\\}^\\mathcal{X} with VC dimensiond_{\\text{VC}}(\\mathcal{H}) &lt; \\infty. Let T_m = \\{(x_i, y_i)\\}_{i=1}^m be i.i.d. samples. Then for any \\delta \\in (0,1), with probability at least 1-\\delta:\n\\forall h \\in \\mathcal{H}:\\quad\nR(h) \\le \\hat R(T_m, h) + 4 \\sqrt{\\frac{d_{\\text{VC}} \\log\\frac{2 e m}{d_{\\text{VC}}} + \\log \\frac{4}{\\delta}}{m}}\nWhere:\n\nR(h) = \\mathbb{E}[[y \\ne h(x)]] = true risk\n\\hat R(T_m, h) = \\frac{1}{m} \\sum_i [[y_i \\ne h(x_i)]] = empirical risk\n\nImplications:\n\nMinimize empirical risk\nControl complexity (VC dimension, sample size)\n"},"Vault/Statistical-machine-learning/Statistical-learning-theory/Vapnik-Chervonenkis-dimension":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/Vapnik-Chervonenkis-dimension","filePath":"Vault/Statistical machine learning/Statistical learning theory/Vapnik-Chervonenkis dimension.md","title":"Vapnik-Chervonenkis dimension","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nLet \\mathcal{H} \\subseteq \\left\\{-1,+1 \\right\\}^{\\mathcal{X}}, the vapnik-Chervonenkis dimension of \\mathcal H, denoted as d_{\\text{VC}}(\\mathcal{H}) is the cardinality of the largest set of points from \\mathcal{X} that can be shattered by \\mathcal{H}.\nThe set of m input points \\{x_1, \\cdots, x_m\\} is shattered by \\mathcal{H}, if for every labeling y \\in \\left\\{-1,+1 \\right\\}^m, there exists a hypothesis h \\in \\mathcal{H} such that\nh(x_i) = y_i.\n\nIn other words, VC-dimension for hypothesis class is the maximum number of points this class can realize - for every possible labeling, there exists a classifier in set |\\mathcal{H}|.\n\nImportant examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass of modelsd_{\\text{VC}}(\\mathcal{H})Linear classifiers from \\mathbb{R}^{d}= d+1line can always separate 3 points, plane can always separate 4 points, …Threshold classifier= 1Any two points with different labels break the threshold if they lie on “wrong side” of the thresholdFinite hypothesis space\\le \\log_{2}{(\\|\\mathcal{H}\\|)}There is 2^m possible labeling or m points. We need at least that many hypothesis to shatter m points.Memorizers\\inftyYou can memorize any number of points.Nearest neighbors classifiers\\inftyTheoretically same as memorizer, we can have infinitely close points for each"},"Vault/Statistical-machine-learning/Statistical-learning-theory/successful-PAC-learner":{"slug":"Vault/Statistical-machine-learning/Statistical-learning-theory/successful-PAC-learner","filePath":"Vault/Statistical machine learning/Statistical learning theory/successful PAC learner.md","title":"successful PAC learner","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nIt is an algorithm, which trains a predictor for given class \\mathcal H. If the following holds, it’s a successful PAC learner.\n\\mathbb{P}_{T_{m} \\sim p^m}(|R(p, h_m) - R(p, h_\\mathcal{H})| \\le \\varepsilon) \\ge 1 - \\deltagiven"},"Vault/Statistical-machine-learning/Support-vector-machines/Multinomial-SVM":{"slug":"Vault/Statistical-machine-learning/Support-vector-machines/Multinomial-SVM","filePath":"Vault/Statistical machine learning/Support vector machines/Multinomial SVM.md","title":"Multinomial SVM","links":[],"tags":[],"content":""},"Vault/Statistical-machine-learning/Support-vector-machines/SVM":{"slug":"Vault/Statistical-machine-learning/Support-vector-machines/SVM","filePath":"Vault/Statistical machine learning/Support vector machines/SVM.md","title":"SVM","links":["tags/SSU","Vault/Statistical-machine-learning/Maximum-margin-classifier","Vault/Statistical-machine-learning/Statistical-learning-theory/Structural-risk-minimization","Hinge-loss","tags/TODO"],"tags":["SSU","TODO"],"content":"SSU | [Metoda podpůrných vektorů]\nReformulation of the Maximum margin classifier problem into a quadratic program solving\n(w^*, b^*) = \n\\text{arg} \\min_{w \\in \\mathbb{R}^d, b \\in \\mathbb{R}}\n\t\t\\frac{1}{2}||w||^2\nsubject to\n\\begin{aligned}\n\\langle x_i, w \\rangle+ b \\ge +1 \\quad i \\in \\mathcal{I}_+\\\\\n\\langle x_i, w \\rangle + b \\le -1 \\quad i \\in \\mathcal{I}_-\n\\end{aligned}\nSupport vectors are the training examples that are closest to the decision hyperplane. These are the only training data we need to keep after training is finished.\nQuestion\n\nDraw a line and a point. Draw a triangle with a right angle and write what the distances are equal to between the line and the point. What is the distance from the margin to other points?\n\nAnswer\n\nQuestion\n\nWhy does minimizing the weight vector maximizes the margin? What is left to minimize when the closest point has distance of 1?\n\nAnswer\nWhen the distance \\langle x_i, w \\rangle+ b = 1, the distance from the decision bound to the support vector is just 1 / ||w||.\nSoft-margin SVM\nWhen data are not linearly separable, we can introduce slack-variables and reformulate the quadratic program to\n(w^*, b^*, \\xi^*) = \n\\text{arg} \\min_{w \\in \\mathbb{R}^d, b \\in \\mathbb{R}, \\xi \\in \\mathbb{R}^m}\n\t\t\\frac{1}{2}||w||^2\n\t\t+\n\t\tC \\sum^{m}_{i = 1}{\\xi_i}\n\nsubject to\n\\begin{aligned}\n\\langle x_i, w \\rangle+ b &amp;\\ge +1-\\xi_i &amp; i \\in \\mathcal{I}_+\\\\\n\\langle x_i, w \\rangle + b &amp;\\le -1+\\xi_i &amp; i \\in \\mathcal{I}_- \\\\\n\\xi_i &amp;\\ge 0 &amp; i \\in \\hat m\n\\end{aligned}\nSlack variable \\xi_i is equal to 0, when point with index i is classified correctly and is positive when misclassified. It “pushes” the point further away from the decision bound, so the point ends up correctly classified.\nRegularization constant C is a hyperparameter. Setting it high pushes slack variables close to zero and tends to overfitting. Setting it low allows greater margin, smoother bound and better generalization.\nImplicit structured risk minimization\nStructural risk minimization is implemented directly inside the algorithm. We can re-define the objective function with Hinge loss as\n(w^*, b^*) = \n\\text{arg} \\min_{w \\in \\mathbb{R}^d, b \\in \\mathbb{R}}\n\t\t\\frac{1}{2}||w||^2\n\t\t+\n\t\tC \\sum^{m}_{i = 1}{\\max\\{0, 1 - \\langle x_i, w \\rangle+ b}\\}\n\nHinge loss is exactly equal to the slack variables.\nIt also implicitly satisfies the conditions that margin is at least 1.\n\nParameter C becomes the complexity term.\nImplementation\nTODO"},"Vault/Statistical-machine-learning/Support-vector-machines/Untitled":{"slug":"Vault/Statistical-machine-learning/Support-vector-machines/Untitled","filePath":"Vault/Statistical machine learning/Support vector machines/Untitled.md","title":"Untitled","links":[],"tags":[],"content":""},"Vault/Statistical-machine-learning/Support-vector-machines/kernel-SVM":{"slug":"Vault/Statistical-machine-learning/Support-vector-machines/kernel-SVM","filePath":"Vault/Statistical machine learning/Support vector machines/kernel SVM.md","title":"kernel SVM","links":["tags/SSU","Vault/Statistical-machine-learning/Support-vector-machines/SVM"],"tags":["SSU"],"content":"SSU\nIn its base form, SVM creates a linear bound. Using kernel functions allows us to create complex non-linear bound, which are still linear in parameters. By using kernel functions, we implicitly map inputs into a feature space.\n\\alpha^* = \\arg\\max_{\\alpha \\in \\mathbb{R}^m}\n\\left[\n\\sum_{i=1}^{m} \\alpha_i\n-\n\\frac{1}{2}\n\\sum_{i=1}^{m}\n\\sum_{j=1}^{m}\n\\alpha_i \\alpha_j y_i y_j k(x_i, x_j)\n\\right]\nsubject to\n\\sum_{i=1}^{m} \\alpha_i y_i = 0,\n\\qquad\n0 \\le \\alpha_i \\le C,\n\\quad\ni \\in I_{+} \\cup I_{-}.\nComputing the"},"Vault/Statistical-machine-learning/Untitled":{"slug":"Vault/Statistical-machine-learning/Untitled","filePath":"Vault/Statistical machine learning/Untitled.md","title":"Untitled","links":[],"tags":[],"content":""},"Vault/Statistics/Central-limit-theorem":{"slug":"Vault/Statistics/Central-limit-theorem","filePath":"Vault/Statistics/Central limit theorem.md","title":"Central limit theorem","links":["tags/SAN"],"tags":["SAN"],"content":"SAN\nCentral Limit Theorem (CLT)\nTheorem: Let X_1, X_2, X_3, \\ldots be a sequence of independent and identically distributed (i.i.d.) random variables with mean \\mu = E[X_i] and finite variance \\sigma^2 = \\text{Var}(X_i) &gt; 0.\nLet S_n = X_1 + X_2 + \\cdots + X_n denote the sum. Then:\n\\frac{S_n - n\\mu}{\\sigma\\sqrt{n}} = \\frac{S_n - E[S_n]}{\\sqrt{\\text{Var}(S_n)}} \\xrightarrow{d} N(0,1)\nwhere \\xrightarrow{d} denotes convergence in distribution.\nEquivalently for the sample mean \\bar{X}_n = \\frac{S_n}{n}:\n\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0,1)\nor:\n\\bar{X}_n \\sim \\text{approximately } N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ for large } n\nKey insight: Standardize by dividing by \\sigma\\sqrt{n} (or \\sigma/\\sqrt{n} for the mean), not just \\sigma.\n\nApplications\n1. Approximating distributions\n\nEven if X_i are not normally distributed, their sum/mean is approximately normal for large n\nRule of thumb: n \\geq 30 often sufficient\n\n2. Confidence intervals\n\nConstruction: \\bar{X}_n \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\n\n3. Hypothesis testing\n\nZ-tests for means with large samples\n\n4. Quality control\n\nProcess monitoring (control charts)\n\n5. Finance\n\nPortfolio returns modeling (sum of many small effects)\n\n6. Monte Carlo simulation\n\nError estimation\n\nWhy it matters: CLT explains why the normal distribution appears so frequently in practice—any phenomenon that results from the sum of many independent factors will be approximately normally distributed."},"Vault/Statistics/Convergence-of-random-variables":{"slug":"Vault/Statistics/Convergence-of-random-variables","filePath":"Vault/Statistics/Convergence of random variables.md","title":"Convergence of random variables","links":[],"tags":[],"content":"Convergence in distribution\nConvergence in probability\nAlmost sure convergence\nConvergence"},"Vault/Statistics/Independent-data":{"slug":"Vault/Statistics/Independent-data","filePath":"Vault/Statistics/Independent data.md","title":"Independent data","links":["tags/SSU"],"tags":["SSU"],"content":"SSU\nRandom variables X_1 and X_2 are independent, iff P[X_1 \\in I_1, X_2 \\in I_2] = P[X_1 \\in I_1]\\cdot P[X_2 \\in I_2] for every I_1, I_2.\n\nThis is a mathematical way of saying that one variable doesn’t affect the other.\n\nEquivalent definition\nRandom variables X_1 and X_2 are independent, iff P[X_1 \\le u_1, X_2 \\le u_2] = P[X_1 \\le u_1]\\cdot P[X_2 \\le u_2] for every I_1, I_2.3"},"Vault/Statistics/Law-of-large-numbers":{"slug":"Vault/Statistics/Law-of-large-numbers","filePath":"Vault/Statistics/Law of large numbers.md","title":"Law of large numbers","links":["tags/SSU","Vault/Statistics/Independent-data","Vault/Statistics/Population-mean","Lebesque-integral","Vault/Statistics/Unbiased-etimator","Vault/Statistics/Convergence-of-random-variables"],"tags":["SSU"],"content":"SSU\nIf data are i.i.d(independent and identically distributed), then their average converges to the true mean.\nStrong LoLN - Let there be an infinite sequence of i.i.d lebesque integrable random variables with equal means E(X_1)=E(X_2)=\\dots = \\mu, than \\frac{1}{n}(X_1+X_2+\\dots+X_n) \\rightarrow \\mu,as n \\rightarrow \\infty.\nWe say that mean is an Unbiased etimator of expected value.\nVariance decays with \\frac{1}{n}.\nWeak LoLN - sample mean converges in probability to true mean."},"Vault/Statistics/Markov's-Inequality":{"slug":"Vault/Statistics/Markov's-Inequality","filePath":"Vault/Statistics/Markov's Inequality.md","title":"Markov's Inequality","links":[],"tags":[],"content":""},"Vault/Statistics/Population-mean":{"slug":"Vault/Statistics/Population-mean","filePath":"Vault/Statistics/Population mean.md","title":"Population mean","links":[],"tags":[],"content":"| true mean"},"Vault/Statistics/Unbiased-etimator":{"slug":"Vault/Statistics/Unbiased-etimator","filePath":"Vault/Statistics/Unbiased etimator.md","title":"Unbiased etimator","links":["tags/SSU"],"tags":["SSU"],"content":"SSU | [nestranný odhad]\nAn estimate of a parameter is called unbiased, when the expected value is equal to the true value of the parameter for large number of samples."},"Vault/Statistics/converges-in-probability":{"slug":"Vault/Statistics/converges-in-probability","filePath":"Vault/Statistics/converges in probability.md","title":"converges in probability","links":[],"tags":[],"content":""},"Vault/Výpočetní-teorie-her/Algorithms/Double-oracle":{"slug":"Vault/Výpočetní-teorie-her/Algorithms/Double-oracle","filePath":"Vault/Výpočetní teorie her/Algorithms/Double oracle.md","title":"Double oracle","links":["tags/MAS","Vault/Výpočetní-teorie-her/Strategic-games/Two-player-zero-sum-games"],"tags":["MAS"],"content":"MAS\nAlgorithm used for matrix games too large to be solved using linear programming approach.\n\nSolve the subgame with T1 and T2 Ð→ (q ∗ 1 , q ∗ 2 )\nCompute the pure best responses s1 ∈ argmax s ′ 1 ∈S1 U(s ′ 1 , q ∗ 2 ) and s2 ∈ argmin s ′ 2 ∈S2 U(q ∗ 1 , s ′ 2 )\nIf s1 ∈ T1 and s2 ∈ T2, then stop\nOtherwise add s1 to T1 or s2 to T2, and go to 1.\n"},"Vault/Výpočetní-teorie-her/Bayesian-game":{"slug":"Vault/Výpočetní-teorie-her/Bayesian-game","filePath":"Vault/Výpočetní teorie her/Bayesian game.md","title":"Bayesian game","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\n\nFor each player i \\in N, there are\n\nType set T_i\nset of actions S_i\nUtility function u_i: \\mathbf{S} \\times \\mathbf{T} \\rightarrow \\mathbb{R}\n\n\nA common prior distribution \\rho on \\mathbf{T}\n\nPlayers know all elements of the game, values of utility, but they might not know the actual type \\mathbf{t} \\in \\mathbf{T}. They only have their believes about it.\nStrategies\nPure strategy of player i is mapping s_i from types to actions, s_i: T_i \\rightarrow S_i\nBehavioral strategy of player i is mapping p_i from types to mixed actions, p_i: T_i \\rightarrow \\Delta_i\nUtilities by types\nEx post\nPlayer i knows types of all players \\mathbf{t}. u_i(\\mathbf{s} | \\mathbf{t}) = u_i(s_1(t_1), s_2(t_2),\\dots, s_n(t_n) | \\mathbf{t})\nInterim\nPlayer i knows only t_i, not \\mathbf{t}_{-i}. u_i(\\mathbf{s}|t_i) = \\sum_{t_i \\in T_i} \\rho(\\mathbf{t}_{-i}|t_i)\\cdot u_i(\\mathbf{s}|\\mathbf{t})\nEx ante\nPlayer doesn’t know any type"},"Vault/Výpočetní-teorie-her/Cooperation-games/Shapley-value":{"slug":"Vault/Výpočetní-teorie-her/Cooperation-games/Shapley-value","filePath":"Vault/Výpočetní teorie her/Cooperation games/Shapley value.md","title":"Shapley value","links":[],"tags":[],"content":""},"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Banzhaf-index":{"slug":"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Banzhaf-index","filePath":"Vault/Výpočetní teorie her/Cooperation games/Voting games/Banzhaf index.md","title":"Banzhaf index","links":[],"tags":[],"content":"normalized Banzhaf index"},"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Shapley-Shubik-index":{"slug":"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Shapley-Shubik-index","filePath":"Vault/Výpočetní teorie her/Cooperation games/Voting games/Shapley-Shubik index.md","title":"Shapley-Shubik index","links":[],"tags":[],"content":""},"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Voting-game":{"slug":"Vault/Výpočetní-teorie-her/Cooperation-games/Voting-games/Voting-game","filePath":"Vault/Výpočetní teorie her/Cooperation games/Voting games/Voting game.md","title":"Voting game","links":[],"tags":[],"content":"Weighed voting game\nTypes of parties in veto games\n\nveto\n"},"Vault/Výpočetní-teorie-her/Definice":{"slug":"Vault/Výpočetní-teorie-her/Definice","filePath":"Vault/Výpočetní teorie her/Definice.md","title":"Definice","links":["tags/MAS","tags/Definice"],"tags":["MAS","Definice"],"content":"MAS Definice\nHra v normálové formě\n\nStrict/weak Domination\nWhat strategy is never played by rational player?\nIterativní odstranění striktně dominovaných strategií\n\nJe tenhle algoritmus jednoznačný pro striktně dominované strategie?\nJe tenhle algoritmus jednoznačný pro slabě dominované strategie?\nDůležité hry\n\nBach or Stravinsky = battle of sexes\nNash equilibrium\n\nPlayer has no incentive to deviate.\nPareto optimální\n"},"Vault/Výpočetní-teorie-her/Equilibria/Best-response":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Best-response","filePath":"Vault/Výpočetní teorie her/Equilibria/Best response.md","title":"Best response","links":["Vault/Výpočetní-teorie-her/Equilibria/Nash-Equilibrium"],"tags":[],"content":"Any strategy s of player i following BR_i(\\mathbf{s}_{-i}) = \\text{argmax}_{s\\in S_i}u_i(s, \\mathbf{s}_{-i}) \\qquad \\forall\\mathbf{s}_{-i} \\in \\mathbf{S}_{-i} is a best response of player i given opponents strategies.\n\nStrategy profile s^* is Nash Equilibrium if and only if s^*_i is best response for each player i.\n\nMixed Best response\nChoose the best probability set from the Mixed strategies simplex (any set of probabilities, which sum is 1).\nBR_i(\\mathbf{p}_{-i}) = \\text{argmax}_{p \\in \\Delta_i}U_i(p, \\mathbf{p}_{-i}) \\qquad \\forall\\mathbf{p}_{-i} \\in \\mathbf{\\Delta}_{-i}"},"Vault/Výpočetní-teorie-her/Equilibria/Correlated-equilibrium":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Correlated-equilibrium","filePath":"Vault/Výpočetní teorie her/Equilibria/Correlated equilibrium.md","title":"Correlated equilibrium","links":["tags/MAS"],"tags":["MAS"],"content":"MAS"},"Vault/Výpočetní-teorie-her/Equilibria/Extensive-form-game-with-imperfect-information":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Extensive-form-game-with-imperfect-information","filePath":"Vault/Výpočetní teorie her/Equilibria/Extensive-form game with imperfect information.md","title":"Extensive-form game with imperfect information","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nIt’s a\n\nThe mediator uses a probability distribution p over S = S_1 × ⋅ ⋅ ⋅ × S_n to generate randomly an action profile s = (s_1, . . . , s_n) ∈ S\nThe mediator tells each player i only s_i\nEach player i is free to choose any action s^′_i ∈ S_i\nThe resulting utility is u_i(s^′_1 , \\dots , s^′_n )\n\nStrategy is a mapping from private recommendations to individual actions. \\sigma_i^*(s_i) = s_j\nExpected utility of player i under (\\sigma_1^*, \\dots, \\sigma_n^*) is \\sum_{\\mathbf{s}_{-i} \\in \\mathbf{S}_{-i}} p(\\mathbf{s}_{-i} | s_{i})\\cdot u_i(s_{i}, \\mathbf{s}_{-i}) \\qquad \\forall s_i \\in S_i\n\nconditional probability that others got recommendations \\mathbf{s}_{-i}, given that you got s_i​\n"},"Vault/Výpočetní-teorie-her/Equilibria/ITERATED-ELIMINATION-OF-DOMINATED-STRATEGIES":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/ITERATED-ELIMINATION-OF-DOMINATED-STRATEGIES","filePath":"Vault/Výpočetní teorie her/Equilibria/ITERATED ELIMINATION OF DOMINATED STRATEGIES.md","title":"ITERATED ELIMINATION OF DOMINATED STRATEGIES","links":["tags/MAS","tags/Algoritmus"],"tags":["MAS","Algoritmus"],"content":"MAS Algoritmus Eliminace dominovaných strategií\nIteratively eliminate dominated strategies until there is no dominated strategy left.\nWeak elimination doesnt give unique result, strict does."},"Vault/Výpočetní-teorie-her/Equilibria/Nash-Equilibrium":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Nash-Equilibrium","filePath":"Vault/Výpočetní teorie her/Equilibria/Nash Equilibrium.md","title":"Nash Equilibrium","links":["tags/MAS","tags/Definice"],"tags":["MAS","Definice"],"content":"MAS Definice\nA strategy profile in which no player has an incentive to deviate, assuming the strategies of all other players remain unchanged.\nDefinition\nPure strategy NE\nA pure strategy profile s^∗ \\in S is a Nash equilibrium if for every player i \\in N u_i(\\mathbf{s}^∗ ) \\ge u_i(s, \\mathbf{s}^∗_{−i} ) \\qquad \\forall s ∈ S_i .\nMixed strategy NE\nA mixed strategy profile p^∗ \\in \\mathbf\\Delta = \\times_{i\\in N} \\Delta_i is a Nash equilibrium if for every player i \\in N U_i(\\mathbf{p}^∗ ) \\ge U_i(p, \\mathbf{p}^∗_{−i} ) \\qquad \\forall p ∈ \\Delta_i ."},"Vault/Výpočetní-teorie-her/Equilibria/Nash's-theorem":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Nash's-theorem","filePath":"Vault/Výpočetní teorie her/Equilibria/Nash's theorem.md","title":"Nash's theorem","links":["tags/MAS","Vault/Výpočetní-teorie-her/Equilibria/Nash-Equilibrium"],"tags":["MAS"],"content":"MAS\nAny finite strategic game has a Nash Equilibrium in mixed strategies.\nPPAD difficult to prove. Only proves existence, finding one is infinitely hard, because of the property of support"},"Vault/Výpočetní-teorie-her/Equilibria/Rational-player":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Rational-player","filePath":"Vault/Výpočetní teorie her/Equilibria/Rational player.md","title":"Rational player","links":["tags/MAS","Vault/Rationalism"],"tags":["MAS"],"content":"MAS racionální hráč\nWe assume that rational players maximize utility\nA person’s behavior is rational if it is in his best interests, given his information. R. Aumann (2006) • The utilitaristic concept of rationality imposes no restrictions on the norms of human behavior Both parties deprecated war; but one would make war rather than let the nation survive; and the other would accept war rather than let it perish. And the war came. A. Lincoln (1865) •"},"Vault/Výpočetní-teorie-her/Equilibria/Untitled":{"slug":"Vault/Výpočetní-teorie-her/Equilibria/Untitled","filePath":"Vault/Výpočetní teorie her/Equilibria/Untitled.md","title":"Untitled","links":[],"tags":[],"content":""},"Vault/Výpočetní-teorie-her/Extensive-form-game/Extensive-form-game":{"slug":"Vault/Výpočetní-teorie-her/Extensive-form-game/Extensive-form-game","filePath":"Vault/Výpočetní teorie her/Extensive-form game/Extensive form game.md","title":"Extensive form game","links":[],"tags":[],"content":"It is an 8-tuple, consisting of\n\nplayer set N = \\{1,\\dots, n\\}\nActions\n"},"Vault/Výpočetní-teorie-her/Important-games/First-price-sealed-bid-auction":{"slug":"Vault/Výpočetní-teorie-her/Important-games/First-price-sealed-bid-auction","filePath":"Vault/Výpočetní teorie her/Important games/First-price sealed-bid auction.md","title":"First-price sealed-bid auction","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nA single item is up for auction. Each buyer submits a (private) bid b_i \\ge 0simultaneously in a sealed envelope. The item is given to the highest bidder who pays their bid.\n\nEvery bidder attaches a private value v_i &gt; 0 to the item.\nThe winner is selected uniformly at random from the k highest bidders.\n"},"Vault/Výpočetní-teorie-her/Important-games/Second-price-sealed-bid-auction":{"slug":"Vault/Výpočetní-teorie-her/Important-games/Second-price-sealed-bid-auction","filePath":"Vault/Výpočetní teorie her/Important games/Second-price sealed-bid auction.md","title":"Second-price sealed-bid auction","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nA single item is up for auction. Each buyer submits a (private) bid b_i \\ge 0simultaneously in a sealed envelope. The item is given to the highest bidder who pays the second-highest bid.\n\n\nEvery bidder attaches a private value v_i &gt; 0 to the item.\n\n\nThe winner is selected uniformly at random from the k highest bidders.\n\n\nWhat is the utility?\n\n\nIs there a nash equilibrium? Is it unique?\n\n"},"Vault/Výpočetní-teorie-her/Indifference-principle":{"slug":"Vault/Výpočetní-teorie-her/Indifference-principle","filePath":"Vault/Výpočetní teorie her/Indifference principle.md","title":"Indifference principle","links":["Vault/Výpočetní-teorie-her/Equilibria/Nash-Equilibrium"],"tags":[],"content":"Let \\mathbf{p}^* be a Nash Equilibrium and i \\in N be any player.\n\nFor every s, t \\in \\text{spt}(p^*_i) U_i(s, \\mathbf{p}_{-i}^*) = U_i(t, \\mathbf{p}_{-i}^*)\nFor every s \\in \\text{spt}(p^*_i) U_i(s, \\mathbf{p}_{-i}^*) = U_i(\\mathbf{p}^*)\n"},"Vault/Výpočetní-teorie-her/Multilinear-feasibility-problem":{"slug":"Vault/Výpočetní-teorie-her/Multilinear-feasibility-problem","filePath":"Vault/Výpočetní teorie her/Multilinear feasibility problem.md","title":"Multilinear feasibility problem","links":[],"tags":[],"content":"Find p1, … , pn satisfying\n\n∑ i∈N (ei − Ui(p ∗ )) ≤ 0\nei − Ui(s, p−i) ≥ 0 for each i ∈ N and every s ∈ Si\npi ∈ ∆i for each i ∈ N\nei ∈ R for each i ∈ N\n"},"Vault/Výpočetní-teorie-her/Pure-math/MinMax-Theorem":{"slug":"Vault/Výpočetní-teorie-her/Pure-math/MinMax-Theorem","filePath":"Vault/Výpočetní teorie her/Pure math/MinMax Theorem.md","title":"MinMax Theorem","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nLet us have sets X, Y and function f : X \\times Y \\rightarrow\\mathbb R. If\n\nX and Y are standard simplexes and\nf is bilinear, f(x,y) = x^TAy , then \\max_{x\\in X}\\min_{y\\in Y} f(x,y) = \\min_{y\\in Y}\\max_{x \\in X} f(x,y)\n"},"Vault/Výpočetní-teorie-her/Pure-math/Support":{"slug":"Vault/Výpočetní-teorie-her/Pure-math/Support","filePath":"Vault/Výpočetní teorie her/Pure math/Support.md","title":"Support","links":[],"tags":[],"content":"Suppose that f:X→R is a real-valued function whose domain is an arbitrary set X. The set-theoretic support of f, written \\text{supp}⁡(f), is the set of points in X where f is non-zero:\n\\text{supp}⁡(f)={x \\in X:f(x)≠0}.\nThe support of f is the smallest subset of X with the property that f is zero on the subset’s complement. If f(x)=0 for all but a finite number of points x∈X, then f is said to have finite support."},"Vault/Výpočetní-teorie-her/Stackelberg-equilibrium":{"slug":"Vault/Výpočetní-teorie-her/Stackelberg-equilibrium","filePath":"Vault/Výpočetní teorie her/Stackelberg equilibrium.md","title":"Stackelberg equilibrium","links":[],"tags":[],"content":"There is a leader and there is\nWeak SE = go against leader = max min U_1\nStrong SE = go with leader = max max U_1"},"Vault/Výpočetní-teorie-her/Strategic-games/Normal-form-game":{"slug":"Vault/Výpočetní-teorie-her/Strategic-games/Normal-form-game","filePath":"Vault/Výpočetní teorie her/Strategic games/Normal form game.md","title":"Normal form game","links":["tags/MAS"],"tags":["MAS"],"content":"MAS Strategic game | hra v normální formě\nTriple (N, (S_i)_{i \\in N}, (u_i)_{i \\in N}) where\n\nN is set of player\nS_i is a set of action of player i\nu_i is a utility function of player i, u_i : \\mathbf{S} \\rightarrow \\mathbb R, where \\mathbf{S} = S_1 \\times S_2 \\times \\dots \\times S_N\n\nThe game proceeds as follows. Each player i discretely chooses from its action set, not knowing the choice of other players. After all players choose their action, strategy profile \\mathbf S is revealed and each player gets award given by its u_i."},"Vault/Výpočetní-teorie-her/Strategic-games/Two-player-zero-sum-games":{"slug":"Vault/Výpočetní-teorie-her/Strategic-games/Two-player-zero-sum-games","filePath":"Vault/Výpočetní teorie her/Strategic games/Two-player zero-sum games.md","title":"Two-player zero-sum games","links":["tags/MAS","Vault/Výpočetní-teorie-her/Strategic-games/Normal-form-game"],"tags":["MAS"],"content":"MAS | Matrix game\nA Strategic game, where\n\nN = {1, 2}\nS_1 and S_2 are finite\nutility function satisfies u_1 = -u_2\n\nWe can view u(s1, s2) as the payoff of player 1/loss of player 2\nTerminology: player 1 is maximizing while player 2 is minimizing\nEasily solvable with dual LP"},"Vault/Výpočetní-teorie-her/Strategies/Behavioral-strategies":{"slug":"Vault/Výpočetní-teorie-her/Strategies/Behavioral-strategies","filePath":"Vault/Výpočetní teorie her/Strategies/Behavioral strategies.md","title":"Behavioral strategies","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nUnlike mixed strategy, behavioral strategy takes into account only the current decision point.\nFormally, it is a mapping \\pi : \\mathcal I \\rightarrow \\Delta A(h)\nwhere \\mathcal I is info set, A(h) is action function of decision node h."},"Vault/Výpočetní-teorie-her/Strategies/Domination-of-Strategy":{"slug":"Vault/Výpočetní-teorie-her/Strategies/Domination-of-Strategy","filePath":"Vault/Výpočetní teorie her/Strategies/Domination of Strategy.md","title":"Domination of Strategy","links":["tags/MAS","Vault/Výpočetní-teorie-her/Strategic-games/Normal-form-game"],"tags":["MAS"],"content":"MAS\nLet s,t ∈ S_i be strategies of player i. We say that\n\nt is strictly dominated by s if u_i(s, s_{−i}) &gt; u_i(t, \\mathbf{s}_{−i}) for every \\mathbf{s}_{−i} ∈ \\mathbf{S}_{−i}\nt is weakly dominated by s if u_i(s, s_{−i}) \\ge u_i(t, \\mathbf{s}_{−i}) for every \\mathbf{s}_{−i} ∈ \\mathbf{S}_{−i},\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLMRU4, 32, 13, 2M1, 43, 22, 3D2, 22, 12, 2\nColumn player:\n\nL vs M: L gives (3, 4, 2), M gives (1, 2, 1)\n\nAgainst U: 3 &gt; 1 ✓\nAgainst M: 4 &gt; 2 ✓\nAgainst D: 2 &gt; 1 ✓\nM is strictly dominated by L\n\n\nL vs R: L gives (3, 4, 2), R gives (2, 3, 2)\n\nAgainst U: 3 &gt; 2 ✓\nAgainst M: 4 &gt; 3 ✓\nAgainst D: 2 = 2 ✓\nR is weakly dominated by L\nRow player:*\nU vs D: U gives (4, 2, 3), D gives (2, 2, 2)\nAgainst L: 4 &gt; 2 ✓\nAgainst M: 2 = 2 ✓\nAgainst R: 3 &gt; 2 ✓\nD is weakly dominated by U\n\n\n"},"Vault/Výpočetní-teorie-her/Strategies/Mixed-strategy":{"slug":"Vault/Výpočetní-teorie-her/Strategies/Mixed-strategy","filePath":"Vault/Výpočetní teorie her/Strategies/Mixed strategy.md","title":"Mixed strategy","links":["tags/MAS"],"tags":["MAS"],"content":"MAS\nMixed strategy of a player i is a probability distribution p_i on S_i.\nExpected utility of a player i is U_i(p_1,\\dots, p_n) = \\sum_{\\mathbf{s}\\in\\mathbf{S}}u_i(\\mathbf{s})\\prod_{j \\in N}p_j(s_j)\nUtility is a multilinear function (special kind if polynomial)\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxyza1,71,53,4b2,30,40,6p := p_1(a)q := p_2(x), r := p_2(y)\nDerive  p_1(b) =1 - p,\nU_1(p,q,r) = 1\\cdot Pr(a,x) + 1\\cdot Pr(ay) + 3\\cdot Pr(az) + 2\\cdot Pr(bx) + 0\\cdot Pr(by) + 0\\cdot  Pr(bz) \\qquad \\quad \\quad = pq + pr + 3p(1-q-r) + 2(1-p)q\nU_2(p,q,r) = 7\\cdot Pr(a,x) + 5\\cdot Pr(ay) + 4\\cdot Pr(az) + 3\\cdot Pr(bx) + 4\\cdot Pr(by) + 6\\cdot Pr(bz)"},"Vault/Výpočetní-teorie-her/Strategies/Pure-strategy":{"slug":"Vault/Výpočetní-teorie-her/Strategies/Pure-strategy","filePath":"Vault/Výpočetní teorie her/Strategies/Pure strategy.md","title":"Pure strategy","links":["tags/MAS"],"tags":["MAS"],"content":"MAS | unanimity principle\nIs a strategy profile, where player always makes the same choice.\n\nalways playing rock\nalways snitch\nalways prepare for a lecture\n\nGood samaritan\nNE only when just one guy helps, other not"},"Vault/index":{"slug":"Vault/index","filePath":"Vault/index.md","title":"index","links":[],"tags":[],"content":"Vault containing my university notes."},"index":{"slug":"index","filePath":"index.md","title":"Welcome to My Blog","links":["Vault","Blog"],"tags":[],"content":"Welcome\nAbout Me\nMy name is Vojtěch Tóth and this is my personal web page.\nYou can find my digital garden and I plan to occasionally write a blog post as an exercise. Check the Vault for my obsidian notes and Blog to read an article. Lately I’ve been working on a minimal torch-like framework for training neural networks from scratch.\nBook list\n\nweb.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\nweb.stanford.edu/~jurafsky/slp3/\ndirect.mit.edu/books/monograph/2604/An-Introduction-to-Computational-Learning-Theory\n"}}